{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Linguistic Degeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have the data, I must think about how to quantify linguistic degeneration. For this, I will be using the following metrics:\n",
    "<br>\n",
    "\n",
    "***Lexical Quality***\n",
    "\n",
    "1. Type-token ratio (TTR): measures vocabulary diversity\n",
    "2. Zipf curve tail heaviness: measures use of rare words\n",
    "3. Average word length: measures use of larger words\n",
    "4. Age of acquisition: measures word difficulty\n",
    "\n",
    "***Syntactic Quality***\n",
    "\n",
    "5. Parse tree depth: measures sentence complexity\n",
    "6. Fragment ratio: measure of informality\n",
    "7. Index of Syntactic Complexity: measures overall syntactic quality\n",
    "\n",
    "***Orthographical Quality***\n",
    "\n",
    "8. Levenshtein distance to dictionary: spelling errors\n",
    "9. Punctuation frequency: attention to grammar\n",
    "\n",
    "***Substantive Quality***\n",
    "\n",
    "10. Abstract concepts\n",
    "11. Figurative language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I do not use other traditional measures of lexical complexity, such as ambiguity, vague quantifier frequency, orthographic neighborhood size, terminology inconsistency. Word ambiguity almost exclusively measures the complexity of the text from the perspective of the reader. As the purpose of this analysis is to look at language degeneration over time as a result of modern phenomena, I am principally interested in the intellect required to produce the complexity, meaning I am looking at true substantive complexity as opposed to inteprative complexity. Similarly, vague quantifier frequency, orthographic neighborhood size, and terminology inconsistency also measure complexity of interpratation, hence they are excluded. It is also worth noting that I categorize abstract concepts and figurative language under substantive quality as opposed to lexical complexity, as they involve more intimately the meaning of the word.\n",
    "\n",
    "Likewise, I exclude certain measures of syntactic complexity, such as sentence length, information overload, passive voice, and negation. In the case of sentence length, it can be a sign of high (i.e., complexity) or low (i.e., run-on sentences) quality writing, which is why I have excluded it. Additionally, sentence complexity can be better measured via ICS. Information overload, passive voice, and negation are excluded for the same interpratability versus quality distinction in the above paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Lexical Quality Metrics With Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample post from the Cornell subreddit for testing tokenizers\n",
    "cornell_example = \"Based on 2017 numbers:    \\n\\n3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039. That gives a 12.7% acceptance rate.    \\n\\nIf Cornell were to enroll 900 more students, that'd be 225 additional students per year.  That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.    \\n\\nKeep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.   \\n\\nRegardless, a 0.8% rise in acceptance doesn't seem too bad.  As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.    \\n\\nSource: http://irp.dpb.cornell.edu/tableau_visual/admissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    '''Helper function to tokenize a string of text, removing non-alphabetic characters'''\n",
    "    \n",
    "    text = text.lower()\n",
    "    processed = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = processed.split()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nicholas $%#! @@is# $@! 123 gre4at.ðŸ‘\n",
      "['nicholas', 'is', 'great']\n",
      "The result should only contain words\n",
      "\n",
      "['based', 'on', 'numbers', 'entering', 'freshman', 'yield', 'meaning', 'were', 'accepted', 'out', 'of', 'that', 'gives', 'a', 'acceptance', 'rate', 'if', 'cornell', 'were', 'to', 'enroll', 'more', 'students', 'thatd', 'be', 'additional', 'students', 'per', 'year', 'that', 'works', 'out', 'to', 'accepted', 'students', 'giving', 'a', 'theoretical', 'acceptance', 'rate', 'of', 'if', 'cornell', 'had', 'implemented', 'this', 'change', 'in', 'keep', 'in', 'mind', 'that', 'this', 'is', 'not', 'an', 'accurate', 'projection', 'for', 'because', 'we', 'get', 'more', 'applicants', 'each', 'year', 'so', 'acceptance', 'rates', 'will', 'actually', 'continue', 'to', 'fall', 'regardless', 'a', 'rise', 'in', 'acceptance', 'doesnt', 'seem', 'too', 'bad', 'as', 'long', 'as', 'the', 'faculty', 'can', 'handle', 'the', 'moderate', 'increase', 'in', 'class', 'sizes', 'and', 'the', 'quality', 'of', 'education', 'stays', 'the', 'same', 'i', 'dont', 'see', 'a', 'reason', 'to', 'reject', 'more', 'people', 'than', 'we', 'have', 'to', 'source', 'httpirpdpbcornelledutableauvisualadmissions']\n"
     ]
    }
   ],
   "source": [
    "example = \"Nicholas $%#! @@is# $@! 123 gre4at.ðŸ‘\"\n",
    "print(example)\n",
    "print(simple_tokenize(example))\n",
    "print('The result should only contain words\\n')\n",
    "print(simple_tokenize(cornell_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the simple tokenizer cannot adequately handle links. See the last index: \"httpirpdpbcornelledutableauvisualadmissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up nltk tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now try the advanced tokenizer\n",
      "Nicholas is great. Hopefully this works. I now have hope. ðŸ‘.\n",
      "['Nicholas', 'is', 'great', '.', 'Hopefully', 'this', 'works', '.', 'I', 'now', 'have', 'hope', '.', 'ðŸ‘', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Now try the advanced tokenizer')\n",
    "another_example = \"Nicholas is great. Hopefully this works. I now have hope. ðŸ‘.\"\n",
    "print(another_example)\n",
    "print(word_tokenize(another_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''Helper function to tokenize social media text. Note that the TweetTokenizer \n",
    "    preserves mentions, contractions'''\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try this on a line from the actual Cornell subreddit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Based',\n",
       " 'on',\n",
       " '2017',\n",
       " 'numbers',\n",
       " ':',\n",
       " '3375',\n",
       " 'entering',\n",
       " 'freshman',\n",
       " ',',\n",
       " '56.6',\n",
       " '%',\n",
       " 'yield',\n",
       " ',',\n",
       " 'meaning',\n",
       " '5962',\n",
       " 'were',\n",
       " 'accepted',\n",
       " 'out',\n",
       " 'of',\n",
       " '47039',\n",
       " '.',\n",
       " 'That',\n",
       " 'gives',\n",
       " 'a',\n",
       " '12.7',\n",
       " '%',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " '.',\n",
       " 'If',\n",
       " 'Cornell',\n",
       " 'were',\n",
       " 'to',\n",
       " 'enroll',\n",
       " '900',\n",
       " 'more',\n",
       " 'students',\n",
       " ',',\n",
       " \"that'd\",\n",
       " 'be',\n",
       " '225',\n",
       " 'additional',\n",
       " 'students',\n",
       " 'per',\n",
       " 'year',\n",
       " '.',\n",
       " 'That',\n",
       " 'works',\n",
       " 'out',\n",
       " 'to',\n",
       " '(',\n",
       " '3375',\n",
       " '+',\n",
       " '225',\n",
       " ')',\n",
       " '/',\n",
       " '0.566',\n",
       " '=',\n",
       " '6360',\n",
       " 'accepted',\n",
       " 'students',\n",
       " ',',\n",
       " 'giving',\n",
       " 'a',\n",
       " 'theoretical',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " 'of',\n",
       " '13.5',\n",
       " '%',\n",
       " 'if',\n",
       " 'Cornell',\n",
       " 'had',\n",
       " 'implemented',\n",
       " 'this',\n",
       " 'change',\n",
       " 'in',\n",
       " '2017',\n",
       " '.',\n",
       " 'Keep',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'an',\n",
       " 'accurate',\n",
       " 'projection',\n",
       " 'for',\n",
       " '2021',\n",
       " 'because',\n",
       " 'we',\n",
       " 'get',\n",
       " '~',\n",
       " '2000',\n",
       " 'more',\n",
       " 'applicants',\n",
       " 'each',\n",
       " 'year',\n",
       " ',',\n",
       " 'so',\n",
       " 'acceptance',\n",
       " 'rates',\n",
       " 'will',\n",
       " 'actually',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'fall',\n",
       " '.',\n",
       " 'Regardless',\n",
       " ',',\n",
       " 'a',\n",
       " '0.8',\n",
       " '%',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'acceptance',\n",
       " \"doesn't\",\n",
       " 'seem',\n",
       " 'too',\n",
       " 'bad',\n",
       " '.',\n",
       " 'As',\n",
       " 'long',\n",
       " 'as',\n",
       " 'the',\n",
       " 'faculty',\n",
       " 'can',\n",
       " 'handle',\n",
       " 'the',\n",
       " 'moderate',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'class',\n",
       " 'sizes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'education',\n",
       " 'stays',\n",
       " 'the',\n",
       " 'same',\n",
       " ',',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'see',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'more',\n",
       " 'people',\n",
       " 'than',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " '.',\n",
       " 'Source',\n",
       " ':',\n",
       " 'http://irp.dpb.cornell.edu/tableau_visual/admissions']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Let's try this on a line from the actual Cornell subreddit\")\n",
    "tokenize(cornell_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lexical_tokens(tokens):\n",
    "    '''Helper function to clean tokens by removing punctuation, numbers, and emojis\n",
    "    for purely lexical analysis.'''\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        # skip over punctuation\n",
    "        if re.match(r'^\\W+$', tok):\n",
    "            continue\n",
    "        # skip over emojis\n",
    "        # if tok.encode()\n",
    "        # only keep alphabetic tokens\n",
    "        if tok.isalpha():\n",
    "            cleaned.append(tok.lower())\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['based',\n",
       " 'on',\n",
       " 'numbers',\n",
       " 'entering',\n",
       " 'freshman',\n",
       " 'yield',\n",
       " 'meaning',\n",
       " 'were',\n",
       " 'accepted',\n",
       " 'out',\n",
       " 'of',\n",
       " 'that',\n",
       " 'gives',\n",
       " 'a',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " 'if',\n",
       " 'cornell',\n",
       " 'were',\n",
       " 'to',\n",
       " 'enroll',\n",
       " 'more',\n",
       " 'students',\n",
       " 'be',\n",
       " 'additional',\n",
       " 'students',\n",
       " 'per',\n",
       " 'year',\n",
       " 'that',\n",
       " 'works',\n",
       " 'out',\n",
       " 'to',\n",
       " 'accepted',\n",
       " 'students',\n",
       " 'giving',\n",
       " 'a',\n",
       " 'theoretical',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'if',\n",
       " 'cornell',\n",
       " 'had',\n",
       " 'implemented',\n",
       " 'this',\n",
       " 'change',\n",
       " 'in',\n",
       " 'keep',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'an',\n",
       " 'accurate',\n",
       " 'projection',\n",
       " 'for',\n",
       " 'because',\n",
       " 'we',\n",
       " 'get',\n",
       " 'more',\n",
       " 'applicants',\n",
       " 'each',\n",
       " 'year',\n",
       " 'so',\n",
       " 'acceptance',\n",
       " 'rates',\n",
       " 'will',\n",
       " 'actually',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'fall',\n",
       " 'regardless',\n",
       " 'a',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'acceptance',\n",
       " 'seem',\n",
       " 'too',\n",
       " 'bad',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'the',\n",
       " 'faculty',\n",
       " 'can',\n",
       " 'handle',\n",
       " 'the',\n",
       " 'moderate',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'class',\n",
       " 'sizes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'education',\n",
       " 'stays',\n",
       " 'the',\n",
       " 'same',\n",
       " 'i',\n",
       " 'see',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'more',\n",
       " 'people',\n",
       " 'than',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'source']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_lexical_tokens(tokenize(cornell_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(text):\n",
    "    '''Function that returns the type-token ratio'''\n",
    "\n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "\n",
    "    # error handling for when there are no tokens\n",
    "    if len(tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # recall that TTR is number of unique words / number of words\n",
    "    num_types = len(set(tokens))\n",
    "    num_tokens = len(tokens)\n",
    "    ttr = num_types / num_tokens\n",
    "\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "The result should be 0.66\n"
     ]
    }
   ],
   "source": [
    "example2 = \"Nicholas is Nicholas\"\n",
    "print(ttr(example2))\n",
    "print('The result should be 0.66')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(text):\n",
    "    '''Function that determines the average word length of a given text'''\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "\n",
    "    # error handling for when there are no tokens\n",
    "    if len(tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    average_length = np.mean([len(word) for word in tokens])\n",
    "\n",
    "    return average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nicholas is great ðŸ‘ &*&()\n",
      "['Nicholas', 'is', 'great', 'ðŸ‘', '&', '*', '&', '(', ')']\n",
      "['nicholas', 'is', 'great']\n",
      "5.0\n",
      "The result should be 5\n"
     ]
    }
   ],
   "source": [
    "example6 = \"\\nNicholas is great ðŸ‘ &*&()\"\n",
    "print(example6)\n",
    "print(tokenize(example6))\n",
    "print(clean_lexical_tokens(tokenize(example6)))\n",
    "\n",
    "print(avg_word_length(example6))\n",
    "print('The result should be 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build aoa_dict: word -> average age of acquisition\n",
    "aoa_df = pd.read_csv(\"Data/KupermanAoAData.csv\")\n",
    "aoa_dict = dict(zip(aoa_df[\"word\"], aoa_df[\"rating_mean\"]))\n",
    "\n",
    "def aoa_score(text, aoa_dict):\n",
    "    '''Returns the average age of acquisition score for a given text'''\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "    aoa_values = [aoa_dict[word] for word in tokens if word in aoa_dict]\n",
    "\n",
    "    # if there are no words, return a default value\n",
    "    if len(aoa_values) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    aoa_score = np.mean(aoa_values)\n",
    "\n",
    "    return aoa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51\n",
      "This result should be lower than:\n",
      "11.163333333333334\n"
     ]
    }
   ],
   "source": [
    "example3 = \"because I am cool\"\n",
    "print(aoa_score(example3, aoa_dict))\n",
    "print('This result should be lower than:')\n",
    "example4 = \"sophisticated technical jargon\"\n",
    "print(aoa_score(example4, aoa_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_score(text):\n",
    "    '''Returns a frequency score (higher -> more frequent) based on the Zipf scale'''\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "    \n",
    "    zipf_values = [zipf_frequency(word, 'en') for word in tokens]\n",
    "\n",
    "     # if there are no words, return a default value\n",
    "    if len(zipf_values) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    zipf_score = np.mean(zipf_values)\n",
    "\n",
    "    return zipf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.012500000000001\n",
      "This result should be higher than:\n",
      "4.013333333333334\n",
      "\n",
      "5.26\n",
      "This result should be 5.26\n"
     ]
    }
   ],
   "source": [
    "print(zipf_score(example3))\n",
    "print('This result should be higher than:')\n",
    "print(zipf_score(example4))\n",
    "print()\n",
    "\n",
    "example5 = \"word\"\n",
    "print(zipf_score(example5))\n",
    "print('This result should be 5.26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Syntactic Quality Metrics With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse-tree depth\n",
    "\n",
    "Fragment ratio\n",
    "\n",
    "Index of Syntactic Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to run the following text block, the following must be run to install the relevant NLP model:\n",
    "<br>\n",
    "**python3 -m spacy download en_core_web_sm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LanguageTool latest:  16%|â–ˆâ–Œ        | 40.1M/255M [00:30<05:13, 686kB/s]"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained English model\n",
    "\n",
    "from nltk import pos_tag\n",
    "import language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    '''Helper function to split a given post into separate sentences'''\n",
    "\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on 2017 numbers:    \n",
      "\n",
      "3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039. That gives a 12.7% acceptance rate.    \n",
      "\n",
      "If Cornell were to enroll 900 more students, that'd be 225 additional students per year.  That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.    \n",
      "\n",
      "Keep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.   \n",
      "\n",
      "Regardless, a 0.8% rise in acceptance doesn't seem too bad.  As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.    \n",
      "\n",
      "Source: http://irp.dpb.cornell.edu/tableau_visual/admissions\n",
      "-----------------------\n",
      "['Based on 2017 numbers:    \\n\\n3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039.', 'That gives a 12.7% acceptance rate.', \"If Cornell were to enroll 900 more students, that'd be 225 additional students per year.\", 'That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.', 'Keep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.', \"Regardless, a 0.8% rise in acceptance doesn't seem too bad.\", \"As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.\", 'Source: http://irp.dpb.cornell.edu/tableau_visual/admissions']\n",
      "\n",
      "Based on 2017 numbers:    \n",
      "\n",
      "3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039.\n",
      "That gives a 12.7% acceptance rate.\n",
      "If Cornell were to enroll 900 more students, that'd be 225 additional students per year.\n",
      "That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.\n",
      "Keep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.\n",
      "Regardless, a 0.8% rise in acceptance doesn't seem too bad.\n",
      "As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.\n",
      "Source: http://irp.dpb.cornell.edu/tableau_visual/admissions\n"
     ]
    }
   ],
   "source": [
    "print(cornell_example)\n",
    "print('-----------------------')\n",
    "\n",
    "split_result = split_sentences(cornell_example)\n",
    "print(split_result)\n",
    "print()\n",
    "for sent in split_result:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following must be run in order to use LanguageTool:\n",
    "<br>\n",
    "\n",
    "**brew install openjdk@17**\n",
    "\n",
    "**sudo ln -sfn $(brew --prefix)/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAGMENT_RULE_IDS = {\"INCOMPLETE_SENTENCE\",\n",
    "                     \"SENTENCE_FRAGMENT\",\n",
    "                     \"EN_SENTENCE_FRAGMENT\",\n",
    "                     \"THIS_MISSING_VERB\"}\n",
    "\n",
    "def is_complete_sentence(sentence):\n",
    "    '''Helper function to determine whether a sentence is complete. Recall that a complete sentence follows these rules:\n",
    "    -contains at least one subject \n",
    "    -contains at least one finite verb\n",
    "    -ends with appropriate punctuation (.?!) \n",
    "    -does not end with a conjunction\n",
    "    -if it begins with a subordinator, has an independent clause after\n",
    "    '''\n",
    "\n",
    "    # first letter should be capital \n",
    "    sentence.strip() # remove any trailing or leading whitespace\n",
    "    first_char = sentence[0]\n",
    "    if not (first_char.isupper()):\n",
    "        # if first char is punctuation, second char should be capital letter\n",
    "        if first_char in '\"\\'({[]})-â€“â€”':\n",
    "            second_char = sentence[1]\n",
    "            if not (second_char.isupper()):\n",
    "                return False\n",
    "        # if first char is not punctuation and not capital\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # must end with proper punctuation\n",
    "    last_char = sentence[-1]\n",
    "    if last_char not in '.!?â€¦':\n",
    "        # if last char is quotation, second to last char should be proper punctuation\n",
    "        if last_char == '\"' or last_char == \"'\":\n",
    "            penultimate_char = sentence[-2]\n",
    "            if penultimate_char not in '.!?â€¦':\n",
    "                return False\n",
    "        # if last char not quotation and not proper punctuation\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # check if sentence is a fragment\n",
    "    matches = tool.check(sentence)\n",
    "    for match in matches:\n",
    "        if match.rule_id in FRAGMENT_RULE_IDS:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LanguageTool.check() got an unexpected keyword argument 'picky'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[165]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      1\u001b[39m tests = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThis a chicken.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn order that\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHe asked if I was coming.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     ]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tests:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     result = \u001b[43mis_complete_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m!s:\u001b[39;00m\u001b[33m5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mis_complete_sentence\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# check if sentence is a fragment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m matches = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpicky\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m match.rule_id \u001b[38;5;129;01min\u001b[39;00m FRAGMENT_RULE_IDS:\n",
      "\u001b[31mTypeError\u001b[39m: LanguageTool.check() got an unexpected keyword argument 'picky'"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"This a chicken.\",\n",
    "    \"In order that\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Although she was tired, she finished her homework.\",\n",
    "    \"Running through the park on a sunny day.\",\n",
    "    \"Because I said so\",\n",
    "    \"Yes.\",\n",
    "    \"What a beautiful day!\",\n",
    "    \"She can do it\",\n",
    "    \"she runs fast.\",\n",
    "    \"He asked if I was coming.\",\n",
    "    ]\n",
    "    \n",
    "for sentence in tests:\n",
    "    result = is_complete_sentence(sentence)\n",
    "    print(f\"{result!s:5} | {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_languagetool(sentence: str):\n",
    "    \"\"\"Print all LanguageTool matches with their rule IDs for a given sentence.\"\"\"\n",
    "    matches = tool.check(sentence)\n",
    "    \n",
    "    print(f\"\\nSentence: {sentence!r}\")\n",
    "    if not matches:\n",
    "        print(\"   â†’ No issues detected (clean!)\")\n",
    "    else:\n",
    "        print(f\"   â†’ Found {len(matches)} issue(s):\")\n",
    "        for i, match in enumerate(matches, 1):\n",
    "            print(f\"     {i}. ruleId: {match.rule_id}\")\n",
    "            print(f\"        message : {match.message}\")\n",
    "            if match.replacements:\n",
    "                print(f\"        suggestions â†’ {match.replacements[:3]}\")  # top 3\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: 'This a chicken.'\n",
      "   â†’ Found 1 issue(s):\n",
      "     1. ruleId: THIS_MISSING_VERB\n",
      "        message : A verb may be missing.\n",
      "        suggestions â†’ ['This is']\n",
      "\n",
      "\n",
      "Sentence: 'In order that'\n",
      "   â†’ No issues detected (clean!)\n",
      "\n",
      "Sentence: 'Yes.'\n",
      "   â†’ No issues detected (clean!)\n",
      "\n",
      "Sentence: 'What a beautiful day!'\n",
      "   â†’ No issues detected (clean!)\n",
      "\n",
      "Sentence: 'Running through the park on a sunny day.'\n",
      "   â†’ No issues detected (clean!)\n",
      "\n",
      "Sentence: 'Because I said so.'\n",
      "   â†’ No issues detected (clean!)\n",
      "\n",
      "Sentence: 'The cat sleeps peacefully.'\n",
      "   â†’ No issues detected (clean!)\n",
      "\n",
      "Sentence: 'she runs fast'\n",
      "   â†’ Found 1 issue(s):\n",
      "     1. ruleId: UPPERCASE_SENTENCE_START\n",
      "        message : This sentence does not start with an uppercase letter.\n",
      "        suggestions â†’ ['She']\n",
      "\n",
      "\n",
      "Sentence: 'Although it was raining'\n",
      "   â†’ No issues detected (clean!)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"This a chicken.\",\n",
    "    \"In order that\",\n",
    "    \"Yes.\",\n",
    "    \"What a beautiful day!\",\n",
    "    \"Running through the park on a sunny day.\",\n",
    "    \"Because I said so.\",\n",
    "    \"The cat sleeps peacefully.\",\n",
    "    \"she runs fast\",\n",
    "    \"Although it was raining\",\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    debug_languagetool(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
