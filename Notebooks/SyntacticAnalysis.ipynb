{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bca17789",
   "metadata": {},
   "source": [
    "\\setcounter{secnumdepth}{0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e249393",
   "metadata": {},
   "source": [
    "## Quantifying Syntactic Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c707221",
   "metadata": {},
   "source": [
    "To quantify syntactic quality, I will rely on the following metrics, the rationale for which can be found in my thesis report:\n",
    "<br>\n",
    "\n",
    "1. Average number of T-units per sentence\n",
    "2. Ratio of clauses to T-units\n",
    "3. Average T-unit length\n",
    "4. Fragment ratio\n",
    "\n",
    "In choosing these metrics, I successfully capture run-on frequency, clausal complexity, and sentence fragment frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f357df9",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8236e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "/Users/nickvick/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 176MB/s]                     \n",
      "2026-01-26 16:09:59 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-01-26 16:09:59 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-01-26 16:10:00 INFO: File exists: /Users/nickvick/stanza_resources/en/default.zip\n",
      "2026-01-26 16:10:01 INFO: Finished downloading models and saved to /Users/nickvick/stanza_resources\n",
      "2026-01-26 16:10:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 45.4MB/s]                    \n",
      "2026-01-26 16:10:01 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-01-26 16:10:01 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-01-26 16:10:01 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2026-01-26 16:10:01 INFO: Using device: cpu\n",
      "2026-01-26 16:10:01 INFO: Loading: tokenize\n",
      "2026-01-26 16:10:02 INFO: Loading: mwt\n",
      "2026-01-26 16:10:02 INFO: Loading: pos\n",
      "2026-01-26 16:10:03 INFO: Loading: constituency\n",
      "2026-01-26 16:10:03 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# syntactic specific imports\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tree import *\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('treebank')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained English model\n",
    "\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "stanza_parser = stanza.Pipeline(\"en\", processors=\"tokenize,pos,constituency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a7dce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nickvick/Library/CloudStorage/OneDrive-PrincetonUniversity/ORFE/Thesis/ORFE-Thesis/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f59bfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for src imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# add project root to sys.path (so src/ can be imported)\n",
    "project_root = os.path.abspath(\"..\")  # adjust if notebooks are nested deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# import required functions\n",
    "from src.data_preprocessing import corpus_to_df, syntactic_preprocessing_df, is_complete_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362c665",
   "metadata": {},
   "source": [
    "## Syntactic Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d1c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant sets of tags and words\n",
    "FINITE_VERB_TAGS = {\"VB\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"}\n",
    "SUBJECT_TAGS = {\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PRP\"}\n",
    "SUBORDINATING_CONJ = {\"IN\"} # tag for subordinating conjunction\n",
    "COORDINATING_CONJ = {\"CC\"} # tag for coordinating conjunction\n",
    "\n",
    "PUNCT = '?!.({[]})-–—\"\\''\n",
    "CLOSING_PUNCT = '.!?…'\n",
    "TRAILING_CLOSERS = set(['\"', \"'\", ')', ']', '}', '”', '’'])\n",
    "\n",
    "# normalize curly quotes and fancy punctuation\n",
    "FANCY_TO_ASCII = {\n",
    "                '“': '\"', '”': '\"',\n",
    "                '‘': \"'\", '’': \"'\",\n",
    "                '—': '-', '–': '-',\n",
    "                '…': '...'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f1023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    '''Helper function to split a given post into separate sentences'''\n",
    "\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f7f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_complete_sentence(sentence):\n",
    "    '''Helper function to determine whether a sentence is complete. Recall that a complete sentence follows these rules:\n",
    "    -contains at least one subject \n",
    "    -contains at least one finite verb\n",
    "    -ends with appropriate punctuation (.?!) \n",
    "    -if it begins with a subordinator, has an independent clause after\n",
    "    -does not end with a conjunction\n",
    "    '''\n",
    "\n",
    "    cleaned = sentence.strip() # removing trailing/leading whitespace\n",
    "    # account for differences in straight vs. smart quotes\n",
    "    for f, a in FANCY_TO_ASCII.items():\n",
    "        cleaned = cleaned.replace(f, a)\n",
    "    # remove leading/trailing quotes\n",
    "    cleaned = cleaned.strip('\\\"')\n",
    "    cleaned = cleaned.strip('\\'')\n",
    "\n",
    "    # empty string\n",
    "    if not cleaned:\n",
    "        return False\n",
    "    \n",
    "    # tokenize sentence and tag tokens\n",
    "    tokens = tokenize(cleaned)\n",
    "    tags = pos_tag(tokens)\n",
    "\n",
    "    # ensure length is appropriate\n",
    "    if len(tokens) < 2:\n",
    "        return False\n",
    "\n",
    "    # first letter should be capital\n",
    "    j = 0\n",
    "    while j < len(cleaned) and cleaned[j] in PUNCT:\n",
    "        j += 1\n",
    "    if j >= len(cleaned):\n",
    "        return False\n",
    "    if not cleaned[j].isalpha() or not cleaned[j].isupper():\n",
    "        return False\n",
    "        \n",
    "    # last relevant char must end with proper punctuation\n",
    "    i = len(cleaned) - 1\n",
    "    while i > 0 and cleaned[i] in TRAILING_CLOSERS:\n",
    "        i -= 1\n",
    "    if i <= 0 or cleaned[i] not in CLOSING_PUNCT:\n",
    "        return False\n",
    "    \n",
    "    # find the first words tag\n",
    "    first_word = None\n",
    "    first_tag = None\n",
    "    for word, tag in tags:\n",
    "        if word.isalpha():\n",
    "            first_word = word\n",
    "            first_tag = tag\n",
    "            break\n",
    "    # if first word is subordinating conjunction (including \"when\"), need independent clause after\n",
    "    if first_tag in SUBORDINATING_CONJ or first_word == \"When\":\n",
    "        if ',' in tokens: # indepdent clause will start after a comma\n",
    "            comma_index = tokens.index(',')\n",
    "            post_sub_tags = tags[comma_index+1:]\n",
    "            # check if independent clause is a complete thought\n",
    "            has_finite_verb_post_sub = any(tag in FINITE_VERB_TAGS for _, tag in post_sub_tags)\n",
    "            has_subject_post_sub = any(tag in SUBJECT_TAGS for _, tag in tags)\n",
    "            if not (has_finite_verb_post_sub and has_subject_post_sub):\n",
    "                return False\n",
    "        # if no comma separating clauses\n",
    "        else:\n",
    "            noun_count = sum(1 for _, tag in tags if tag in SUBJECT_TAGS)\n",
    "            verb_count = sum(1 for _, tag in tags if tag in FINITE_VERB_TAGS)\n",
    "            # edge case for when first word is if\n",
    "            if first_word == \"If\" and verb_count < 2:\n",
    "                return False\n",
    "            # check for two nouns, if not assume fragment\n",
    "            if noun_count < 2:\n",
    "                return False\n",
    "\n",
    "    # find the last words tag\n",
    "    last_tag = None\n",
    "    for word, tag in reversed(tags):\n",
    "        if word.isalpha():\n",
    "            last_tag = tag\n",
    "            break\n",
    "    # last word cannot be conjunction\n",
    "    if last_tag in COORDINATING_CONJ:\n",
    "        return False\n",
    "\n",
    "    # check if it has finite verb and subject\n",
    "    has_finite_verb = any(tag in FINITE_VERB_TAGS for _, tag in tags)\n",
    "    has_subject = any(tag in SUBJECT_TAGS for _, tag in tags)\n",
    "\n",
    "    return has_finite_verb and has_subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b98f7f",
   "metadata": {},
   "source": [
    "## Syntactic Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d26f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_ratio(text):\n",
    "    '''Function to determine the ratio of fragments to lines in a given text'''\n",
    "\n",
    "    sentences = split_sentences(text)\n",
    "    total = len(sentences)\n",
    "    if total == 0:\n",
    "        return None\n",
    "\n",
    "    # add complete sentences to a list\n",
    "    is_complete = []\n",
    "    for sent in sentences:\n",
    "        if is_complete_sentence(sent):\n",
    "            is_complete.append(sent)\n",
    "\n",
    "    num_fragment = total - len(is_complete)\n",
    "\n",
    "    fragment_ratio = num_fragment/total\n",
    "\n",
    "    return fragment_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7246720e",
   "metadata": {},
   "source": [
    "## Corpus Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582aa9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=\"../subreddit-teenagers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4efcca8",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb797c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
