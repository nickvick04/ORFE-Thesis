{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\setcounter{secnumdepth}{0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Linguistic Degeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have the data, I must think about how to quantify linguistic degeneration. For this, I will be using the following metrics:\n",
    "<br>\n",
    "\n",
    "***Lexical Quality***\n",
    "\n",
    "1. Type-token ratio (TTR): measures vocabulary diversity\n",
    "2. Zipf curve tail heaviness: measures use of rare words\n",
    "3. Average word length: measures use of larger words\n",
    "4. Age of acquisition: measures word difficulty\n",
    "\n",
    "***Syntactic Quality***\n",
    "\n",
    "5. Parse tree depth: measures sentence complexity\n",
    "6. Fragment ratio: measure of informality\n",
    "7. Index of Syntactic Complexity: measures overall syntactic quality\n",
    "\n",
    "***Orthographical Quality***\n",
    "\n",
    "8. Levenshtein distance to dictionary: spelling errors\n",
    "9. Punctuation frequency: attention to grammar\n",
    "\n",
    "***Substantive Quality***\n",
    "\n",
    "10. Abstract concepts\n",
    "11. Figurative language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I do not use other traditional measures of lexical complexity, such as ambiguity, vague quantifier frequency, orthographic neighborhood size, terminology inconsistency. Word ambiguity almost exclusively measures the complexity of the text from the perspective of the reader. As the purpose of this analysis is to look at language degeneration over time as a result of modern phenomena, I am principally interested in the intellect required to produce the complexity, meaning I am looking at true substantive complexity as opposed to inteprative complexity. Similarly, vague quantifier frequency, orthographic neighborhood size, and terminology inconsistency also measure complexity of interpratation, hence they are excluded. It is also worth noting that I categorize abstract concepts and figurative language under substantive quality as opposed to lexical complexity, as they involve more intimately the meaning of the word.\n",
    "\n",
    "Likewise, I exclude certain measures of syntactic complexity, such as sentence length, information overload, passive voice, and negation. In the case of sentence length, it can be a sign of high (i.e., complexity) or low (i.e., run-on sentences) quality writing, which is why I have excluded it. Additionally, sentence complexity can be better measured via ICS. Information overload, passive voice, and negation are excluded for the same interpratability versus quality distinction in the above paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Lexical Quality Metrics With Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample post from the Cornell subreddit for testing tokenizers\n",
    "cornell_example = \"Based on 2017 numbers:    \\n\\n3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039. That gives a 12.7% acceptance rate.    \\n\\nIf Cornell were to enroll 900 more students, that'd be 225 additional students per year.  That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.    \\n\\nKeep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.   \\n\\nRegardless, a 0.8% rise in acceptance doesn't seem too bad.  As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.    \\n\\nSource: http://irp.dpb.cornell.edu/tableau_visual/admissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    '''Helper function to tokenize a string of text, removing non-alphabetic characters'''\n",
    "    \n",
    "    text = text.lower()\n",
    "    processed = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = processed.split()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nicholas $%#! @@is# $@! 123 gre4at.üëç\n",
      "['nicholas', 'is', 'great']\n",
      "The result should only contain words\n",
      "\n",
      "['based', 'on', 'numbers', 'entering', 'freshman', 'yield', 'meaning', 'were', 'accepted', 'out', 'of', 'that', 'gives', 'a', 'acceptance', 'rate', 'if', 'cornell', 'were', 'to', 'enroll', 'more', 'students', 'thatd', 'be', 'additional', 'students', 'per', 'year', 'that', 'works', 'out', 'to', 'accepted', 'students', 'giving', 'a', 'theoretical', 'acceptance', 'rate', 'of', 'if', 'cornell', 'had', 'implemented', 'this', 'change', 'in', 'keep', 'in', 'mind', 'that', 'this', 'is', 'not', 'an', 'accurate', 'projection', 'for', 'because', 'we', 'get', 'more', 'applicants', 'each', 'year', 'so', 'acceptance', 'rates', 'will', 'actually', 'continue', 'to', 'fall', 'regardless', 'a', 'rise', 'in', 'acceptance', 'doesnt', 'seem', 'too', 'bad', 'as', 'long', 'as', 'the', 'faculty', 'can', 'handle', 'the', 'moderate', 'increase', 'in', 'class', 'sizes', 'and', 'the', 'quality', 'of', 'education', 'stays', 'the', 'same', 'i', 'dont', 'see', 'a', 'reason', 'to', 'reject', 'more', 'people', 'than', 'we', 'have', 'to', 'source', 'httpirpdpbcornelledutableauvisualadmissions']\n"
     ]
    }
   ],
   "source": [
    "example = \"Nicholas $%#! @@is# $@! 123 gre4at.üëç\"\n",
    "print(example)\n",
    "print(simple_tokenize(example))\n",
    "print('The result should only contain words\\n')\n",
    "print(simple_tokenize(cornell_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the simple tokenizer cannot adequately handle links. See the last index: \"httpirpdpbcornelledutableauvisualadmissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up nltk tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now try the advanced tokenizer\n",
      "Nicholas is great. Hopefully this works. I now have hope. üëç.\n",
      "['Nicholas', 'is', 'great', '.', 'Hopefully', 'this', 'works', '.', 'I', 'now', 'have', 'hope', '.', 'üëç', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Now try the advanced tokenizer')\n",
    "another_example = \"Nicholas is great. Hopefully this works. I now have hope. üëç.\"\n",
    "print(another_example)\n",
    "print(word_tokenize(another_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''Helper function to tokenize social media text. Note that the TweetTokenizer \n",
    "    preserves mentions, contractions'''\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try this on a line from the actual Cornell subreddit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Based',\n",
       " 'on',\n",
       " '2017',\n",
       " 'numbers',\n",
       " ':',\n",
       " '3375',\n",
       " 'entering',\n",
       " 'freshman',\n",
       " ',',\n",
       " '56.6',\n",
       " '%',\n",
       " 'yield',\n",
       " ',',\n",
       " 'meaning',\n",
       " '5962',\n",
       " 'were',\n",
       " 'accepted',\n",
       " 'out',\n",
       " 'of',\n",
       " '47039',\n",
       " '.',\n",
       " 'That',\n",
       " 'gives',\n",
       " 'a',\n",
       " '12.7',\n",
       " '%',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " '.',\n",
       " 'If',\n",
       " 'Cornell',\n",
       " 'were',\n",
       " 'to',\n",
       " 'enroll',\n",
       " '900',\n",
       " 'more',\n",
       " 'students',\n",
       " ',',\n",
       " \"that'd\",\n",
       " 'be',\n",
       " '225',\n",
       " 'additional',\n",
       " 'students',\n",
       " 'per',\n",
       " 'year',\n",
       " '.',\n",
       " 'That',\n",
       " 'works',\n",
       " 'out',\n",
       " 'to',\n",
       " '(',\n",
       " '3375',\n",
       " '+',\n",
       " '225',\n",
       " ')',\n",
       " '/',\n",
       " '0.566',\n",
       " '=',\n",
       " '6360',\n",
       " 'accepted',\n",
       " 'students',\n",
       " ',',\n",
       " 'giving',\n",
       " 'a',\n",
       " 'theoretical',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " 'of',\n",
       " '13.5',\n",
       " '%',\n",
       " 'if',\n",
       " 'Cornell',\n",
       " 'had',\n",
       " 'implemented',\n",
       " 'this',\n",
       " 'change',\n",
       " 'in',\n",
       " '2017',\n",
       " '.',\n",
       " 'Keep',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'an',\n",
       " 'accurate',\n",
       " 'projection',\n",
       " 'for',\n",
       " '2021',\n",
       " 'because',\n",
       " 'we',\n",
       " 'get',\n",
       " '~',\n",
       " '2000',\n",
       " 'more',\n",
       " 'applicants',\n",
       " 'each',\n",
       " 'year',\n",
       " ',',\n",
       " 'so',\n",
       " 'acceptance',\n",
       " 'rates',\n",
       " 'will',\n",
       " 'actually',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'fall',\n",
       " '.',\n",
       " 'Regardless',\n",
       " ',',\n",
       " 'a',\n",
       " '0.8',\n",
       " '%',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'acceptance',\n",
       " \"doesn't\",\n",
       " 'seem',\n",
       " 'too',\n",
       " 'bad',\n",
       " '.',\n",
       " 'As',\n",
       " 'long',\n",
       " 'as',\n",
       " 'the',\n",
       " 'faculty',\n",
       " 'can',\n",
       " 'handle',\n",
       " 'the',\n",
       " 'moderate',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'class',\n",
       " 'sizes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'education',\n",
       " 'stays',\n",
       " 'the',\n",
       " 'same',\n",
       " ',',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'see',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'more',\n",
       " 'people',\n",
       " 'than',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " '.',\n",
       " 'Source',\n",
       " ':',\n",
       " 'http://irp.dpb.cornell.edu/tableau_visual/admissions']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Let's try this on a line from the actual Cornell subreddit\")\n",
    "tokenize(cornell_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lexical_tokens(tokens):\n",
    "    '''Helper function to clean tokens by removing punctuation, numbers, and emojis\n",
    "    for purely lexical analysis.'''\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for tok in tokens:\n",
    "        # skip over punctuation\n",
    "        if re.match(r'^\\W+$', tok):\n",
    "            continue\n",
    "        # skip over emojis\n",
    "        # if tok.encode()\n",
    "        # only keep alphabetic tokens\n",
    "        if tok.isalpha():\n",
    "            cleaned.append(tok.lower())\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['based',\n",
       " 'on',\n",
       " 'numbers',\n",
       " 'entering',\n",
       " 'freshman',\n",
       " 'yield',\n",
       " 'meaning',\n",
       " 'were',\n",
       " 'accepted',\n",
       " 'out',\n",
       " 'of',\n",
       " 'that',\n",
       " 'gives',\n",
       " 'a',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " 'if',\n",
       " 'cornell',\n",
       " 'were',\n",
       " 'to',\n",
       " 'enroll',\n",
       " 'more',\n",
       " 'students',\n",
       " 'be',\n",
       " 'additional',\n",
       " 'students',\n",
       " 'per',\n",
       " 'year',\n",
       " 'that',\n",
       " 'works',\n",
       " 'out',\n",
       " 'to',\n",
       " 'accepted',\n",
       " 'students',\n",
       " 'giving',\n",
       " 'a',\n",
       " 'theoretical',\n",
       " 'acceptance',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'if',\n",
       " 'cornell',\n",
       " 'had',\n",
       " 'implemented',\n",
       " 'this',\n",
       " 'change',\n",
       " 'in',\n",
       " 'keep',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'an',\n",
       " 'accurate',\n",
       " 'projection',\n",
       " 'for',\n",
       " 'because',\n",
       " 'we',\n",
       " 'get',\n",
       " 'more',\n",
       " 'applicants',\n",
       " 'each',\n",
       " 'year',\n",
       " 'so',\n",
       " 'acceptance',\n",
       " 'rates',\n",
       " 'will',\n",
       " 'actually',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'fall',\n",
       " 'regardless',\n",
       " 'a',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'acceptance',\n",
       " 'seem',\n",
       " 'too',\n",
       " 'bad',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'the',\n",
       " 'faculty',\n",
       " 'can',\n",
       " 'handle',\n",
       " 'the',\n",
       " 'moderate',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'class',\n",
       " 'sizes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'education',\n",
       " 'stays',\n",
       " 'the',\n",
       " 'same',\n",
       " 'i',\n",
       " 'see',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'more',\n",
       " 'people',\n",
       " 'than',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'source']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_lexical_tokens(tokenize(cornell_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(text):\n",
    "    '''Function that returns the type-token ratio'''\n",
    "\n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "\n",
    "    # error handling for when there are no tokens\n",
    "    if len(tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # recall that TTR is number of unique words / number of words\n",
    "    num_types = len(set(tokens))\n",
    "    num_tokens = len(tokens)\n",
    "    ttr = num_types / num_tokens\n",
    "\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "The result should be 0.66\n"
     ]
    }
   ],
   "source": [
    "example2 = \"Nicholas is Nicholas\"\n",
    "print(ttr(example2))\n",
    "print('The result should be 0.66')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(text):\n",
    "    '''Function that determines the average word length of a given text'''\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "\n",
    "    # error handling for when there are no tokens\n",
    "    if len(tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    average_length = np.mean([len(word) for word in tokens])\n",
    "\n",
    "    return average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nicholas is great üëç &*&()\n",
      "['Nicholas', 'is', 'great', 'üëç', '&', '*', '&', '(', ')']\n",
      "['nicholas', 'is', 'great']\n",
      "5.0\n",
      "The result should be 5\n"
     ]
    }
   ],
   "source": [
    "example6 = \"\\nNicholas is great üëç &*&()\"\n",
    "print(example6)\n",
    "print(tokenize(example6))\n",
    "print(clean_lexical_tokens(tokenize(example6)))\n",
    "\n",
    "print(avg_word_length(example6))\n",
    "print('The result should be 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build aoa_dict: word -> average age of acquisition\n",
    "aoa_df = pd.read_csv(\"../Data/KupermanAoAData.csv\")\n",
    "aoa_dict = dict(zip(aoa_df[\"word\"], aoa_df[\"rating_mean\"]))\n",
    "\n",
    "def aoa_score(text, aoa_dict):\n",
    "    '''Returns the average age of acquisition score for a given text'''\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "    aoa_values = [aoa_dict[word] for word in tokens if word in aoa_dict]\n",
    "\n",
    "    # if there are no words, return a default value\n",
    "    if len(aoa_values) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    aoa_score = np.mean(aoa_values)\n",
    "\n",
    "    return aoa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51\n",
      "This result should be lower than:\n",
      "11.163333333333334\n"
     ]
    }
   ],
   "source": [
    "example3 = \"because I am cool\"\n",
    "print(aoa_score(example3, aoa_dict))\n",
    "print('This result should be lower than:')\n",
    "example4 = \"sophisticated technical jargon\"\n",
    "print(aoa_score(example4, aoa_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_score(text):\n",
    "    '''Returns a frequency score (higher -> more frequent) based on the Zipf scale'''\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    tokens = clean_lexical_tokens(tokens)\n",
    "    \n",
    "    zipf_values = [zipf_frequency(word, 'en') for word in tokens]\n",
    "\n",
    "     # if there are no words, return a default value\n",
    "    if len(zipf_values) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    zipf_score = np.mean(zipf_values)\n",
    "\n",
    "    return zipf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.012500000000001\n",
      "This result should be higher than:\n",
      "4.013333333333334\n",
      "\n",
      "5.26\n",
      "This result should be 5.26\n"
     ]
    }
   ],
   "source": [
    "print(zipf_score(example3))\n",
    "print('This result should be higher than:')\n",
    "print(zipf_score(example4))\n",
    "print()\n",
    "\n",
    "example5 = \"word\"\n",
    "print(zipf_score(example5))\n",
    "print('This result should be 5.26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Syntactic Quality Metrics With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to run the following text block, the following must be run to install the relevant NLP model:\n",
    "<br>\n",
    "**python3 -m spacy download en_core_web_sm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tree import *\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('treebank')\n",
    "\n",
    "import spacy, benepar\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickvick/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 186MB/s]                     \n",
      "2026-01-27 09:47:07 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-01-27 09:47:07 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-01-27 09:47:08 INFO: File exists: /Users/nickvick/stanza_resources/en/default.zip\n",
      "2026-01-27 09:47:09 INFO: Finished downloading models and saved to /Users/nickvick/stanza_resources\n",
      "2026-01-27 09:47:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 42.2MB/s]                    \n",
      "2026-01-27 09:47:09 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-01-27 09:47:09 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-01-27 09:47:10 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2026-01-27 09:47:10 INFO: Using device: cpu\n",
      "2026-01-27 09:47:10 INFO: Loading: tokenize\n",
      "2026-01-27 09:47:10 INFO: Loading: mwt\n",
      "2026-01-27 09:47:10 INFO: Loading: pos\n",
      "2026-01-27 09:47:11 INFO: Loading: constituency\n",
      "2026-01-27 09:47:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "stanza_parser = stanza.Pipeline(\"en\", processors=\"tokenize,pos,constituency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    '''Helper function to split a given post into separate sentences'''\n",
    "\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on 2017 numbers:    \n",
      "\n",
      "3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039. That gives a 12.7% acceptance rate.    \n",
      "\n",
      "If Cornell were to enroll 900 more students, that'd be 225 additional students per year.  That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.    \n",
      "\n",
      "Keep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.   \n",
      "\n",
      "Regardless, a 0.8% rise in acceptance doesn't seem too bad.  As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.    \n",
      "\n",
      "Source: http://irp.dpb.cornell.edu/tableau_visual/admissions\n",
      "-----------------------\n",
      "['Based on 2017 numbers:    \\n\\n3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039.', 'That gives a 12.7% acceptance rate.', \"If Cornell were to enroll 900 more students, that'd be 225 additional students per year.\", 'That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.', 'Keep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.', \"Regardless, a 0.8% rise in acceptance doesn't seem too bad.\", \"As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.\", 'Source: http://irp.dpb.cornell.edu/tableau_visual/admissions']\n",
      "\n",
      "Based on 2017 numbers:    \n",
      "\n",
      "3375 entering freshman, 56.6% yield, meaning 5962 were accepted out of 47039.\n",
      "That gives a 12.7% acceptance rate.\n",
      "If Cornell were to enroll 900 more students, that'd be 225 additional students per year.\n",
      "That works out to (3375+225)/0.566 = 6360 accepted students, giving a theoretical acceptance rate of 13.5% if Cornell had implemented this change in 2017.\n",
      "Keep in mind that this is not an accurate projection for 2021 because we get ~2000 more applicants each year, so acceptance rates will actually continue to fall.\n",
      "Regardless, a 0.8% rise in acceptance doesn't seem too bad.\n",
      "As long as the faculty can handle the moderate increase in class sizes and the quality of education stays the same, I don't see a reason to reject more people than we have to.\n",
      "Source: http://irp.dpb.cornell.edu/tableau_visual/admissions\n"
     ]
    }
   ],
   "source": [
    "print(cornell_example)\n",
    "print('-----------------------')\n",
    "\n",
    "split_result = split_sentences(cornell_example)\n",
    "print(split_result)\n",
    "print()\n",
    "for sent in split_result:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant sets of tags and words\n",
    "FINITE_VERB_TAGS = {\"VB\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"}\n",
    "SUBJECT_TAGS = {\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PRP\"}\n",
    "SUBORDINATING_CONJ_TAGS = {\"IN\"} # tag for subordinating conjunction\n",
    "COORDINATING_CONJ_TAGS = {\"CC\"} # tag for coordinating conjunction\n",
    "SUBORDINATE_CLAUSE_MARKERS = {\"that\", \"which\", \"who\", \"whom\", \"whose\"}\n",
    "\n",
    "PUNCT = '?!.,:;({[]})-‚Äì‚Äî\"\\''\n",
    "CLOSING_PUNCT = '.!?‚Ä¶'\n",
    "TRAILING_CLOSERS = set(['\"', \"'\", ')', ']', '}', '‚Äù', '‚Äô'])\n",
    "\n",
    "# normalize curly quotes and fancy punctuation\n",
    "FANCY_TO_ASCII = {\n",
    "                '‚Äú': '\"', '‚Äù': '\"',\n",
    "                '‚Äò': \"'\", '‚Äô': \"'\",\n",
    "                '‚Äî': '-', '‚Äì': '-',\n",
    "                '‚Ä¶': '...'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_complete_sentence(sentence):\n",
    "    '''Helper function to determine whether a sentence is complete. Recall that a complete sentence follows these rules:\n",
    "    -contains at least one subject \n",
    "    -contains at least one finite verb\n",
    "    -ends with appropriate punctuation (.?!) \n",
    "    -if it begins with a subordinator, has an independent clause after\n",
    "    -does not end with a conjunction\n",
    "    '''\n",
    "\n",
    "    cleaned = sentence.strip() # removing trailing/leading whitespace\n",
    "    # account for differences in straight vs. smart quotes\n",
    "    for f, a in FANCY_TO_ASCII.items():\n",
    "        cleaned = cleaned.replace(f, a)\n",
    "    # remove leading/trailing quotes\n",
    "    cleaned = cleaned.strip('\\\"')\n",
    "    cleaned = cleaned.strip('\\'')\n",
    "\n",
    "    # empty string\n",
    "    if not cleaned:\n",
    "        return False\n",
    "    \n",
    "    # tokenize sentence and tag tokens\n",
    "    tokens = tokenize(cleaned)\n",
    "    tags = pos_tag(tokens)\n",
    "\n",
    "    # ensure length is appropriate\n",
    "    if len(tokens) < 2:\n",
    "        return False\n",
    "\n",
    "    # first letter should be capital\n",
    "    j = 0\n",
    "    while j < len(cleaned) and cleaned[j] in PUNCT:\n",
    "        j += 1\n",
    "    if j >= len(cleaned):\n",
    "        return False\n",
    "    if not cleaned[j].isalpha() or not cleaned[j].isupper():\n",
    "        return False\n",
    "        \n",
    "    # last relevant char must end with proper punctuation\n",
    "    i = len(cleaned) - 1\n",
    "    while i > 0 and cleaned[i] in TRAILING_CLOSERS:\n",
    "        i -= 1\n",
    "    if i <= 0 or cleaned[i] not in CLOSING_PUNCT:\n",
    "        return False\n",
    "    \n",
    "    # find the first words tag\n",
    "    first_word = None\n",
    "    first_tag = None\n",
    "    for word, tag in tags:\n",
    "        if word.isalpha():\n",
    "            first_word = word\n",
    "            first_tag = tag\n",
    "            break\n",
    "    # if first word is subordinating conjunction (including \"when\"), need independent clause after\n",
    "    if first_tag in SUBORDINATING_CONJ_TAGS or first_word == \"When\":\n",
    "        if ',' in tokens: # indepdent clause will start after a comma\n",
    "            comma_index = tokens.index(',')\n",
    "            post_sub_tags = tags[comma_index+1:]\n",
    "            # check if independent clause is a complete thought\n",
    "            has_finite_verb_post_sub = any(tag in FINITE_VERB_TAGS for _, tag in post_sub_tags)\n",
    "            has_subject_post_sub = any(tag in SUBJECT_TAGS for _, tag in tags)\n",
    "            if not (has_finite_verb_post_sub and has_subject_post_sub):\n",
    "                return False\n",
    "        # if no comma separating clauses\n",
    "        else:\n",
    "            noun_count = sum(1 for _, tag in tags if tag in SUBJECT_TAGS)\n",
    "            verb_count = sum(1 for _, tag in tags if tag in FINITE_VERB_TAGS)\n",
    "            # edge case for when first word is if\n",
    "            if first_word == \"If\" and verb_count < 2:\n",
    "                return False\n",
    "            # check for two nouns, if not assume fragment\n",
    "            if noun_count < 2:\n",
    "                return False\n",
    "\n",
    "    # find the last words tag\n",
    "    last_tag = None\n",
    "    for word, tag in reversed(tags):\n",
    "        if word.isalpha():\n",
    "            last_tag = tag\n",
    "            break\n",
    "    # last word cannot be conjunction\n",
    "    if last_tag in COORDINATING_CONJ_TAGS:\n",
    "        return False\n",
    "\n",
    "    # check if it has finite verb and subject\n",
    "    has_finite_verb = any(tag in FINITE_VERB_TAGS for _, tag in tags)\n",
    "    has_subject = any(tag in SUBJECT_TAGS for _, tag in tags)\n",
    "\n",
    "    return has_finite_verb and has_subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following should result in true\n",
      "True  | Where did he go?\n",
      "True  | The quick brown fox jumps over the lazy dog.\n",
      "True  | Although she was tired, she finished her homework.\n",
      "True  | He asked if I was coming.\n",
      "True  | \"I was there.\"\n",
      "True  | 'I was there.'\n",
      "True  | ‚ÄúI don‚Äôt know what you mean,‚Äù she said.\n",
      "True  | The sign read ‚ÄúNo parking after 6 PM.‚Äù\n",
      "True  | She said, ‚ÄúAfter the storm ended.‚Äù\n",
      "True  | (After a long day,) he went straight to bed.\n",
      "True  | Despite the rain, the game continued.\n",
      "True  | Wait! Are you sure this is the right address?\n",
      "True  | Yes, I understand the instructions.\n",
      "True  | John, who had been waiting for hours, finally boarded the train.\n",
      "True  | The committee approved the proposal‚Äîafter much debate.\n",
      "True  | In the end, everything turned out well.\n",
      "True  | They arrived at 7 PM; the meeting began shortly after.\n",
      "True  | ‚ÄúStop!‚Äù the officer shouted.\n",
      "True  | Because it was late, they decided to head home.\n",
      "True  | If you want my opinion, that was the right choice.\n",
      "True  | No one knew where the noise came from.\n",
      "True  | In the end, he thought it was cool, but it was not.\n",
      "True  | In the end it was cool.\n",
      "True  | Because she was exhausted, she went to bed early.\n",
      "True  | Although the results were surprising, the conclusion was clear.\n",
      "True  | The main reason was clear: no one had prepared.\n",
      "True  | One important issue remained: funding was insufficient.\n",
      "True  | She was tired; she still finished the assignment.\n",
      "True  | The weather was terrible; the game continued anyway.\n",
      "\n",
      "The following should result in false\n",
      "False | In the end talk.\n",
      "False | Running down the street.\n",
      "False | Because she was tired.\n",
      "False | Although it was raining.\n",
      "False | If you need anything.\n",
      "False | Yes!\n",
      "False | Maybe.\n",
      "False | So we went.\n",
      "False | Or maybe not.\n",
      "False | \"After the meeting.\"\n",
      "False | ‚ÄúBefore the storm.‚Äù\n",
      "False | When he arrived.\n",
      "False | If possible.\n",
      "False | Such as this example.\n",
      "False | To the store.\n",
      "False | Under the old bridge.\n",
      "False | The big red barn.\n",
      "False | (Before the show.)\n",
      "False | While waiting for the bus.\n",
      "False | Because she was exhausted,\n",
      "False | Although the results were surprising,\n",
      "False | The main reason:\n",
      "False | One important issue:\n",
      "False | Because she was tired;\n",
      "False | Although it seemed unlikely;\n",
      "False | Because she was tired, after working all night\n",
      "False | Although the results were promising, according to the report\n",
      "False | The main issue was: a lack of preparation\n",
      "False | One thing became clear: during the final review\n",
      "False | Because she was exhausted; after working all night\n",
      "False | Although it seemed reasonable; given the circumstances\n"
     ]
    }
   ],
   "source": [
    "complete_tests = [\n",
    "    \"Where did he go?\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Although she was tired, she finished her homework.\",\n",
    "    \"He asked if I was coming.\",\n",
    "    \"\\\"I was there.\\\"\",\n",
    "    \"'I was there.'\",\n",
    "    \"‚ÄúI don‚Äôt know what you mean,‚Äù she said.\",\n",
    "    \"The sign read ‚ÄúNo parking after 6 PM.‚Äù\",\n",
    "    \"She said, ‚ÄúAfter the storm ended.‚Äù\",\n",
    "    \"(After a long day,) he went straight to bed.\",\n",
    "    \"Despite the rain, the game continued.\",\n",
    "    \"Wait! Are you sure this is the right address?\",\n",
    "    \"Yes, I understand the instructions.\",\n",
    "    \"John, who had been waiting for hours, finally boarded the train.\",\n",
    "    \"The committee approved the proposal‚Äîafter much debate.\",\n",
    "    \"In the end, everything turned out well.\",\n",
    "    \"They arrived at 7 PM; the meeting began shortly after.\",\n",
    "    \"‚ÄúStop!‚Äù the officer shouted.\",\n",
    "    \"Because it was late, they decided to head home.\",\n",
    "    \"If you want my opinion, that was the right choice.\",\n",
    "    \"No one knew where the noise came from.\",\n",
    "    \"In the end, he thought it was cool, but it was not.\",\n",
    "    \"In the end it was cool.\",\n",
    "    \"Because she was exhausted, she went to bed early.\",\n",
    "    \"Although the results were surprising, the conclusion was clear.\",\n",
    "    \"The main reason was clear: no one had prepared.\",\n",
    "    \"One important issue remained: funding was insufficient.\",\n",
    "    \"She was tired; she still finished the assignment.\",\n",
    "    \"The weather was terrible; the game continued anyway.\"\n",
    "]\n",
    "\n",
    "incomplete_tests = [\n",
    "    \"In the end talk.\",\n",
    "    \"Running down the street.\",\n",
    "    \"Because she was tired.\",\n",
    "    \"Although it was raining.\",\n",
    "    \"If you need anything.\",\n",
    "    \"Yes!\",\n",
    "    \"Maybe.\",\n",
    "    \"So we went.\",\n",
    "    \"Or maybe not.\",\n",
    "    \"\\\"After the meeting.\\\"\",\n",
    "    \"‚ÄúBefore the storm.‚Äù\",\n",
    "    \"When he arrived.\",\n",
    "    \"If possible.\",\n",
    "    \"Such as this example.\",\n",
    "    \"To the store.\",\n",
    "    \"Under the old bridge.\",\n",
    "    \"The big red barn.\",\n",
    "    \"(Before the show.)\",\n",
    "    \"While waiting for the bus.\",\n",
    "    \"Because she was exhausted,\",\n",
    "    \"Although the results were surprising,\",\n",
    "    \"The main reason:\",\n",
    "    \"One important issue:\",\n",
    "    \"Because she was tired;\",\n",
    "    \"Although it seemed unlikely;\",\n",
    "    \"Because she was tired, after working all night\",\n",
    "    \"Although the results were promising, according to the report\",\n",
    "    \"The main issue was: a lack of preparation\",\n",
    "    \"One thing became clear: during the final review\",\n",
    "    \"Because she was exhausted; after working all night\",\n",
    "    \"Although it seemed reasonable; given the circumstances\"\n",
    "]\n",
    "    \n",
    "print(\"The following should result in true\")\n",
    "for sentence in complete_tests:\n",
    "    result_true = is_complete_sentence(sentence)\n",
    "    print(f\"{result_true!s:5} | {sentence}\")\n",
    "\n",
    "\n",
    "print(\"\\nThe following should result in false\")\n",
    "for sentence in incomplete_tests:\n",
    "    result_false = is_complete_sentence(sentence)\n",
    "    print(f\"{result_false!s:5} | {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_ratio(text):\n",
    "    '''Function to determine the ratio of fragments to lines in a given text'''\n",
    "\n",
    "    sentences = split_sentences(text)\n",
    "    total = len(sentences)\n",
    "    if total == 0:\n",
    "        return None\n",
    "\n",
    "    # add complete sentences to a list\n",
    "    is_complete = []\n",
    "    for sent in sentences:\n",
    "        if is_complete_sentence(sent):\n",
    "            is_complete.append(sent)\n",
    "\n",
    "    num_fragment = total - len(is_complete)\n",
    "\n",
    "    fragment_ratio = num_fragment/total\n",
    "\n",
    "    return fragment_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "The following should print 0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(fragment_ratio(cornell_example))\n",
    "fragment_example = \"I just got back from hiking. Weather bad. We ate chicken. In order to drive. (This is an interesting example.) If you want. He said, \\\"Try some of this.\\\" Okay!\"\n",
    "print(\"The following should print 0.5\")\n",
    "print(fragment_ratio(fragment_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nltk_tree(sentence):\n",
    "    '''Helper function to create a tree for a valid sentence'''\n",
    "\n",
    "    '''if not is_complete_sentence(sentence):\n",
    "        raise ValueError(\"Sentence is not complete\")'''\n",
    "    \n",
    "    doc = stanza_parser(sentence)\n",
    "    stanza_tree = doc.sentences[0].constituency\n",
    "    nltk_tree = Tree.fromstring(str(stanza_tree))\n",
    "    \n",
    "    return nltk_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (FRAG (PP (IN In) (NP (PRP$ my) (JJ honest) (NN opinion))) (. .)))\n",
      "              ROOT             \n",
      "               |                \n",
      "              FRAG             \n",
      "           ____|_____________   \n",
      "          PP                 | \n",
      "  ________|____              |  \n",
      " |             NP            | \n",
      " |    _________|_______      |  \n",
      " IN PRP$       JJ      NN    . \n",
      " |   |         |       |     |  \n",
      " In  my      honest opinion  . \n",
      "\n",
      "(ROOT\n",
      "  (S\n",
      "    (PP (IN In) (NP (PRP$ my) (JJ honest) (NN opinion)))\n",
      "    (, ,)\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBP do)\n",
      "      (RB not)\n",
      "      (VP\n",
      "        (VB have)\n",
      "        (NP (NP (NN time)) (PP (IN for) (NP (DT this) (NN play))))))\n",
      "    (. .)))\n",
      "                                    ROOT                                        \n",
      "                                     |                                           \n",
      "                                     S                                          \n",
      "           __________________________|________________________________________   \n",
      "          |                  |   |                 VP                         | \n",
      "          |                  |   |    _____________|____                      |  \n",
      "          |                  |   |   |    |             VP                    | \n",
      "          |                  |   |   |    |    _________|___                  |  \n",
      "          |                  |   |   |    |   |             NP                | \n",
      "          |                  |   |   |    |   |     ________|___              |  \n",
      "          PP                 |   |   |    |   |    |            PP            | \n",
      "  ________|____              |   |   |    |   |    |     _______|____         |  \n",
      " |             NP            |   NP  |    |   |    NP   |            NP       | \n",
      " |    _________|_______      |   |   |    |   |    |    |        ____|___     |  \n",
      " IN PRP$       JJ      NN    ,  PRP VBP   RB  VB   NN   IN      DT       NN   . \n",
      " |   |         |       |     |   |   |    |   |    |    |       |        |    |  \n",
      " In  my      honest opinion  ,   I   do  not have time for     this     play  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "example7 = \"In my honest opinion.\"\n",
    "tree = create_nltk_tree(example7)\n",
    "print(tree)\n",
    "print(TreePrettyPrinter(tree).text())\n",
    "\n",
    "example8 = \"In my honest opinion, I do not have time for this play.\"\n",
    "tree = create_nltk_tree(example8)\n",
    "print(tree)\n",
    "print(TreePrettyPrinter(tree).text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_tree_depth(text):\n",
    "    '''Function to compute the average depth of the parse tree representing each sentence.\n",
    "    Note that .height() accounts for the leaf level, which is not a true extra layer.'''\n",
    "\n",
    "    sentences = split_sentences(text)\n",
    "\n",
    "    # for each sentence/fragment, compute the tree height/depth\n",
    "    depths = []\n",
    "    for sent in sentences:\n",
    "        tree = create_nltk_tree(sent)\n",
    "        depths.append(tree.height()-1)\n",
    "\n",
    "    avg_tree_depth = np.mean(depths)\n",
    "    \n",
    "    return avg_tree_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following should print 6\n",
      "6.0\n",
      "The following should print 9\n",
      "9.0\n",
      "\n",
      "In my honest opinion. In my honest opinion, I do not have time for this play.\n",
      "['In my honest opinion.', 'In my honest opinion, I do not have time for this play.']\n",
      "The following should print 7.5\n",
      "7.5\n"
     ]
    }
   ],
   "source": [
    "print(\"The following should print 6\")\n",
    "print(avg_tree_depth(example7))\n",
    "print(\"The following should print 9\")\n",
    "print(avg_tree_depth(example8))\n",
    "print()\n",
    "\n",
    "example9 = example7 + \" \" + example8\n",
    "print(example9)\n",
    "print(split_sentences(example9))\n",
    "print(\"The following should print 7.5\")\n",
    "print(avg_tree_depth(example9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ROOT                   \n",
      "             |                      \n",
      "             S                     \n",
      "  ___________|___________________   \n",
      " |                VP             | \n",
      " |      __________|___           |  \n",
      " NP    |              NP         | \n",
      " |     |      ________|_____     |  \n",
      "NNP   VBD    DT       JJ    NN   . \n",
      " |     |     |        |     |    |  \n",
      "Risa bought  a       blue watch  . \n",
      "\n",
      "             ROOT                         \n",
      "              |                            \n",
      "              S                           \n",
      "  ____________|_________________________   \n",
      " |            VP                        | \n",
      " |    ________|__________________       |  \n",
      " |   |        PP                 |      | \n",
      " |   |     ___|____              |      |  \n",
      " NP  |    |        NP            NP     | \n",
      " |   |    |    ____|____         |      |  \n",
      "PRP VBD   IN  DT        NN       NN     . \n",
      " |   |    |   |         |        |      |  \n",
      " He went  to the      market yesterday  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "example10 = \"Risa bought a blue watch.\"\n",
    "Risa = create_nltk_tree(example10)\n",
    "print(TreePrettyPrinter(Risa).text())\n",
    "\n",
    "example11 = \"He went to the market yesterday.\"\n",
    "market = create_nltk_tree(example11)\n",
    "print(TreePrettyPrinter(market).text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBORDINATE_RELS = {\"advcl\", \"ccomp\", \"xcomp\", \"acl\", \"relcl\"}\n",
    "CLAUSE_RELS = {\"advcl\", \"ccomp\", \"xcomp\", \"acl\", \"relcl\", \"csubj\", \"csubj:pass\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_subordinate_clauses(text):\n",
    "    '''Helper function to compute the number of subordinate clauses present among the complete sentences of a given text.'''\n",
    "\n",
    "    # extract complete sentences only\n",
    "    candidate_sentences = split_sentences\n",
    "    complete_sentences = [sent for sent in candidate_sentences\n",
    "                          if is_complete_sentence(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_t_units(sentence):\n",
    "\n",
    "    # filter out fragments\n",
    "    if not is_complete_sentence(sentence):\n",
    "        return 0\n",
    "    \n",
    "    # there must be at least one independent clause if sentence is complete\n",
    "    t_units = 1\n",
    "\n",
    "    # tokenize and tag sentence\n",
    "    tokens = tokenize(sentence)\n",
    "    tags = pos_tag(tokens)\n",
    "\n",
    "    # initialize booleans\n",
    "    in_subordinate = False\n",
    "\n",
    "    for i, (word, tag) in enumerate(tags):\n",
    "        print(f\"current word:{word}\")\n",
    "\n",
    "        # enter subordinate clause\n",
    "        if tag in SUBORDINATING_CONJ_TAGS and word not in {\"so\"}:\n",
    "            print(\"enter subordinate\")\n",
    "            in_subordinate = True\n",
    "            continue\n",
    "        if word in SUBORDINATE_CLAUSE_MARKERS:\n",
    "            print(\"enter subordinate\")\n",
    "            in_subordinate = True\n",
    "            continue\n",
    "\n",
    "        # semicolons always separate independent clauses\n",
    "        if word == \";\":\n",
    "            for _, next_tag in tags[i+1:]:\n",
    "                if next_tag in FINITE_VERB_TAGS:\n",
    "                    t_units += 1\n",
    "                    break\n",
    "            continue\n",
    "        \n",
    "        # exit subordinate clause at punctuation\n",
    "        if word in PUNCT:\n",
    "            print(\"exit at punct\")\n",
    "            in_subordinate = False\n",
    "            continue\n",
    "            \n",
    "\n",
    "        # coordinating conjunctions often introduce new t-units\n",
    "        if not in_subordinate:\n",
    "            if tag in COORDINATING_CONJ_TAGS or word == \"so\":\n",
    "                print(\"coordinating conj and not subordinate\")\n",
    "\n",
    "                # initialize booleans\n",
    "                found_subject = False\n",
    "\n",
    "                # look ahead for a subject and finite verb\n",
    "                for next_word, next_tag in tags[i+1:]:\n",
    "\n",
    "                    # clause boundary reached so stop searching\n",
    "                    if next_word in {\",\", \";\"}:\n",
    "                        print(\"clause boundary reached\")\n",
    "                        break\n",
    "                    \n",
    "                    # search for subject\n",
    "                    if not found_subject:\n",
    "                        if next_tag in SUBJECT_TAGS:\n",
    "                            print(\"found subject\")\n",
    "                            found_subject = True\n",
    "                        continue\n",
    "\n",
    "                    # check if followed by finite verb\n",
    "                    if next_tag in FINITE_VERB_TAGS:\n",
    "                        print(\"found full t-unit\")\n",
    "                        t_units += 1\n",
    "                        break\n",
    "        \n",
    "        # commas can sometimes introduce independent clauses\n",
    "        if word == \",\" and not in_subordinate:\n",
    "            print(\"word is comma\")\n",
    "\n",
    "            # initialize booleans\n",
    "            found_subject = False\n",
    "\n",
    "            for next_word, next_tag in tags[i+1:]:\n",
    "                # clause boundary reached so stop searching\n",
    "                if next_word in {\",\", \";\"}:\n",
    "                    break\n",
    "                \n",
    "                # search for subject\n",
    "                if not found_subject:\n",
    "                    if next_tag in SUBJECT_TAGS:\n",
    "                        found_subject = True\n",
    "                    continue\n",
    "\n",
    "                # check if followed by finite verb\n",
    "                if next_tag in FINITE_VERB_TAGS:\n",
    "                    t_units += 1\n",
    "                    break\n",
    "\n",
    "    return t_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_UNIT_TESTS = [\n",
    "    # single T-unit (simple / subordinate only)\n",
    "    (\"I left.\", 1),\n",
    "    (\"I left because it was late.\", 1),\n",
    "    (\"When it was late, I left.\", 1),\n",
    "    (\"I left when it was late.\", 1),\n",
    "    (\"The man who lives here is nice.\", 1),\n",
    "    (\"I think that he knows.\", 1),\n",
    "\n",
    "    # two T-units (coordinated independent clauses)\n",
    "    (\"I left, and I went home.\", 2),\n",
    "    (\"I left, but I went home later.\", 2),\n",
    "    (\"I left and I went home.\", 2),\n",
    "    (\"I studied, so I passed.\", 2),\n",
    "\n",
    "    # coordination + subordination\n",
    "    (\"I left, and I went home because it was late.\", 2),\n",
    "    (\"When it was late, I left, and I went home.\", 2),\n",
    "    (\"I left because it was late, and I went home.\", 2),\n",
    "\n",
    "    # three T-units\n",
    "    (\"I came, I saw, and I conquered.\", 3),\n",
    "    (\"I left, and I went home, but I forgot my keys.\", 3),\n",
    "\n",
    "    # relative clauses should NOT increase T-units\n",
    "    (\"The man who lives here and works nearby is nice.\", 1),\n",
    "    (\"I saw the book that you mentioned, and I bought it.\", 2),\n",
    "\n",
    "    # semicolon-separated independent clauses\n",
    "    (\"I left; I went home.\", 2),\n",
    "    (\"I left; I went home; I slept.\", 3),\n",
    "\n",
    "    # edge cases\n",
    "    (\"I left and went home.\", 1),  # shared subject\n",
    "    (\"I left, and then I went home.\", 2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import ParentedTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_unit_counter(sentence):\n",
    "\n",
    "    t_unit_count = 0\n",
    "\n",
    "    # if a fragment, there are no t-units\n",
    "    if not is_complete_sentence(sentence):\n",
    "        return 0\n",
    "    \n",
    "    # create a dependency tree\n",
    "    tree = create_nltk_tree(sentence)\n",
    "    ptree = ParentedTree.convert(tree)\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "        # check for subjects\n",
    "        if subtree.label() == \"S\":\n",
    "            # if subject belongs to subordinate clause, ignore\n",
    "            if subtree.parent().label() == \"SBAR\":\n",
    "                continue\n",
    "            # otherwise increment\n",
    "            else:\n",
    "                t_unit_count += 1\n",
    "    \n",
    "    # if more than one t-unit, ignore duplicated subject below root\n",
    "    if t_unit_count > 1:\n",
    "        t_unit_count -= 1\n",
    "\n",
    "    return t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import ParentedTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "SBAR found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_unit_counter(\"I left, and I went home because it was late.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_t_unit_counter():\n",
    "    failures = []\n",
    "\n",
    "    for sent, expected in T_UNIT_TESTS:\n",
    "        actual = t_unit_counter(sent)\n",
    "        if actual != expected:\n",
    "            failures.append((sent, expected, actual))\n",
    "\n",
    "    if not failures:\n",
    "        print(\"All T-unit tests passed.\")\n",
    "    else:\n",
    "        print(\"Failures detected:\\n\")\n",
    "        for sent, exp, act in failures:\n",
    "            print(f\"Sentence: {sent}\")\n",
    "            print(f\"Expected: {exp}, Got: {act}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incrementing\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "incrementing\n",
      "SBAR found\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "incrementing\n",
      "All T-unit tests passed.\n"
     ]
    }
   ],
   "source": [
    "test_t_unit_counter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
