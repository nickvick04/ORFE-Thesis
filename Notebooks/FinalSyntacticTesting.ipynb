{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "/Users/nickvick/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 256MB/s]                     \n",
      "2026-02-03 15:25:21 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-02-03 15:25:21 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-02-03 15:25:22 INFO: File exists: /Users/nickvick/stanza_resources/en/default.zip\n",
      "2026-02-03 15:25:23 INFO: Finished downloading models and saved to /Users/nickvick/stanza_resources\n",
      "2026-02-03 15:25:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 40.9MB/s]                    \n",
      "2026-02-03 15:25:23 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-02-03 15:25:23 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-02-03 15:25:24 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2026-02-03 15:25:24 INFO: Using device: cpu\n",
      "2026-02-03 15:25:24 INFO: Loading: tokenize\n",
      "2026-02-03 15:25:24 INFO: Loading: mwt\n",
      "2026-02-03 15:25:24 INFO: Loading: pos\n",
      "2026-02-03 15:25:25 INFO: Loading: constituency\n",
      "2026-02-03 15:25:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# syntactic specific imports\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tree import ParentedTree\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('treebank')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained English model\n",
    "\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "stanza_parser = stanza.Pipeline(\"en\", processors=\"tokenize,pos,constituency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# set up for src imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# add project root to sys.path (so src/ can be imported)\n",
    "project_root = os.path.abspath(\"..\")  # adjust if notebooks are nested deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# import required functions\n",
    "from src.data_preprocessing import corpus_to_df, syntactic_preprocessing_df, is_complete_sentence, clean_tokens_lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/nickvick/.convokit/saved-corpora/subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"subreddit-Cornell\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parented_tree(sentence):\n",
    "    '''Helper function to create a tree for a valid sentence'''\n",
    "\n",
    "    '''if not is_complete_sentence(sentence):\n",
    "        raise ValueError(\"Sentence is not complete\")'''\n",
    "    \n",
    "    doc = stanza_parser(sentence)\n",
    "    stanza_tree = doc.sentences[0].constituency\n",
    "    parented_tree = ParentedTree.fromstring(str(stanza_tree))\n",
    "    \n",
    "    return parented_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_t_units(sentence):\n",
    "\n",
    "    t_unit_count = 0\n",
    "    is_question = False\n",
    "    counted_s_label = False\n",
    "    has_nested_sq_label = False\n",
    "\n",
    "    # if a fragment, there are no t-units\n",
    "    if not is_complete_sentence(sentence):\n",
    "        return 0\n",
    "    \n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(sentence)\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "\n",
    "        # extract relevant labels\n",
    "        label = subtree.label()\n",
    "        if subtree.parent():\n",
    "            parent_label = subtree.parent().label()\n",
    "\n",
    "        # flag if the sentence is a question and thus has different rules\n",
    "        if label in {\"SQ\", \"SBARQ\"}:\n",
    "             is_question = True\n",
    "             # if we've counted a preceding S label decrement\n",
    "             if counted_s_label:\n",
    "                 t_unit_count -= 1\n",
    "                 counted_s_label = False\n",
    "\n",
    "        # logic if sentence is a question\n",
    "        if is_question:\n",
    "            if label == \"SQ\":\n",
    "                t_unit_count += 1\n",
    "                # if nested SQ label, flag\n",
    "                if parent_label == \"SQ\":\n",
    "                    has_nested_sq_label = True\n",
    "\n",
    "        # logic when sentence is not a question\n",
    "        else:\n",
    "            # subtract occurences when \"to\" is considered a new subject\n",
    "            if label == \"TO\":\n",
    "                t_unit_count -= 1\n",
    "                    \n",
    "            # check for subjects in regular sentences\n",
    "            if label == \"S\":\n",
    "                # if subject belongs to subordinate clause, ignore\n",
    "                if parent_label == \"SBAR\":\n",
    "                    continue\n",
    "                # otherwise increment\n",
    "                counted_s_label = True\n",
    "                t_unit_count += 1\n",
    "    \n",
    "    # ignore duplicated subject labels\n",
    "    if t_unit_count > 1 and not is_question:\n",
    "        t_unit_count -= 1\n",
    "    if has_nested_sq_label:\n",
    "        t_unit_count -= 1\n",
    "\n",
    "    return t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clauses(sentence):\n",
    "\n",
    "    clause_count = 0\n",
    "\n",
    "    # only consider complete sentences\n",
    "    if not is_complete_sentence(sentence):\n",
    "        return 0\n",
    "    \n",
    "    t_unit_count = count_t_units(sentence)\n",
    "\n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(sentence)\n",
    "    # print(TreePrettyPrinter(ptree))\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "        # if subject belongs to subordinate clause, increment \n",
    "        if subtree.label() == \"SBAR\":\n",
    "            clause_count += 1\n",
    "\n",
    "    return clause_count + t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_unit_length(sentence):\n",
    "    '''Computes the t-unit length (i.e., the number of words divided by number of t-units)'''\n",
    "\n",
    "    # only consider complete sentences\n",
    "    if not is_complete_sentence(sentence):\n",
    "        return 0\n",
    "\n",
    "    tokens = clean_tokens_lexical(sentence)\n",
    "    num_words = len(tokens)\n",
    "\n",
    "    num_t_units = count_t_units(sentence)\n",
    "\n",
    "    return num_words / num_t_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_ratio(text):\n",
    "    '''Function to determine the ratio of fragments to lines in a given text'''\n",
    "\n",
    "    sentences = split_sentences(text)\n",
    "    total = len(sentences)\n",
    "    if total == 0:\n",
    "        return None\n",
    "\n",
    "    # add complete sentences to a list\n",
    "    is_complete = []\n",
    "    for sent in sentences:\n",
    "        if is_complete_sentence(sent):\n",
    "            is_complete.append(sent)\n",
    "\n",
    "    num_fragment = total - len(is_complete)\n",
    "\n",
    "    fragment_ratio = num_fragment/total\n",
    "\n",
    "    return fragment_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_syntactic_vals(df):\n",
    "    '''Function to compute the syntactic metrics for each utterance in a dataframe.'''\n",
    "\n",
    "    avg_t_units_list = []\n",
    "    clause_t_unit_ratio_list = []\n",
    "    avg_t_unit_length_list = []\n",
    "\n",
    "    for utterance_sentences in tqdm(df[\"final\"]):\n",
    "\n",
    "        if len(utterance_sentences) == 0:\n",
    "            continue\n",
    "\n",
    "        print(utterance_sentences)\n",
    "\n",
    "        # list of values for each sentence\n",
    "        t_units_per_sent = [count_t_units(s) for s in utterance_sentences]\n",
    "        clauses_per_sent = [count_clauses(s) for s in utterance_sentences]\n",
    "        t_unit_lengths_per_sent = [t_unit_length(s) for s in utterance_sentences]\n",
    "\n",
    "        if sum(t_units_per_sent) == 0:\n",
    "            continue\n",
    "        if sum(t_unit_lengths_per_sent) == 0:\n",
    "            continue\n",
    "\n",
    "        # average per utterance\n",
    "        avg_t_units = sum(t_units_per_sent) / len(t_units_per_sent)\n",
    "        avg_clause_t_unit_ratio = sum(clauses_per_sent) / sum(t_units_per_sent)\n",
    "        avg_t_unit_len = sum(t_unit_lengths_per_sent) / len(t_unit_lengths_per_sent)\n",
    "\n",
    "        # store values for the current utterance\n",
    "        avg_t_units_list.append(avg_t_units)\n",
    "        clause_t_unit_ratio_list.append(avg_clause_t_unit_ratio)\n",
    "        avg_t_unit_length_list.append(avg_t_unit_len)\n",
    "\n",
    "    # store all values in dataframe\n",
    "    df[\"avg_t_units\"] = avg_t_units_list\n",
    "    df[\"clause_to_t_unit_ratio\"] = clause_t_unit_ratio_list\n",
    "    df[\"avg_t_unit_length\"] = avg_t_unit_length_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickvick/Library/CloudStorage/OneDrive-PrincetonUniversity/ORFE/Thesis/ORFE-Thesis/src/data_preprocessing.py:312: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[\"text\"].str.contains(BOT_TEXT_RE, regex=True)]\n"
     ]
    }
   ],
   "source": [
    "df = corpus_to_df(corpus)\n",
    "df = syntactic_preprocessing_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65796 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I was just reading about the Princeton Mic-Check and it's getting [national press](URL).\", 'I want to get a sense of what people felt like around campus.', 'Anything interesting happen?', 'Anything interesting coming up?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/65796 [00:00<16:51:09,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have added support for Cornell to courseoff.com (URL).', 'Courseoff is a free web app to help you plan your semester schedules.', 'It is very popular with students at some of the other schools I support.', 'No signup is required to use it so feel free to try it out!', 'You can create an account which allows multiple schedules, saving schedules, and sharing schedules.', 'Let me know what you guys think!', 'Any feedback is always appreciated.', 'If you like it, tell your friends :) If you find a problem, let me know as well.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/65796 [00:03<37:56:11,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i don't have a facebook, so we'd need a volunteer.. just someone to let cornell on facebook know that we have a presence on reddit.. perhaps a small explanation of what reddit is?\", 'now that we are almost beautiful and such.. we need more redditors!']\n",
      "[\"so, i'm starting to mess with some of the css on our lovely subreddit.. anyone have any fun suggestions about our little envelope?\", 'or up/downvote things?', 'GO NUTS.']\n",
      "['Ever since SOPA put fear into the hearts of everyone that loves the internet, it looks like [The DarkNet Plan](URL) has grown by the thousands and even got [national media attention](URL).', 'What is the feasibility of doing that for our big red campus?', 'Relevant: I miss DC++']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/65796 [00:04<15:52:19,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm seriously considering cornell for law school, and this intrigues me.\", 'it\\'s not at all a part of my decision making process, but the fact that cornell was at some point by some people known as \"godless cornell\" (i think i read that on wikipedia) makes me smile.', 'it seems like most of the older schools in this country are religious and cornell is non-sectarian (i believe).', 'how does this play out at the school?', \"edit: to be more clear, i guess i'm wondering if the student body is especially agnostic/atheist leaning or if there's some sort of unstated distaste for religion.\", \"i'm agnostic atheist FWIW.\"]\n",
      "[\"So I signed with Cornell as a swimmer a couple of months ago, and as a freshman-to-be I'm beginning put some thought into my future living situations.\", 'As a swimmer I can choose between sharing a townhouse with three others, or taking a dorm.', 'I went on a recruit trip back in October and got to see the townhouse, but only for a little bit.', 'I was wondering if anyone had any opinions on the townhouse vs. a dorm.', \"Anything is greatly appreciated, and I can't wait to represent Big Red next year!\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/65796 [00:06<16:23:51,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey this forum!', 'I was wondering what the bars were like in Ithaca during the winter session when the undergrads are gone.', 'Is collegetown empty but the commons still alive?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/65796 [00:07<15:40:38,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What do you think of Cornell's big crackdown on drinking?\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/65796 [00:07<17:59:07,  1.02it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcompute_syntactic_vals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcompute_syntactic_vals\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     16\u001b[39m t_units_per_sent = [count_t_units(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m utterance_sentences]\n\u001b[32m     17\u001b[39m clauses_per_sent = [count_clauses(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m utterance_sentences]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m t_unit_lengths_per_sent = [\u001b[43mt_unit_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m utterance_sentences]\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(t_units_per_sent) == \u001b[32m0\u001b[39m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mt_unit_length\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m      9\u001b[39m num_words = \u001b[38;5;28mlen\u001b[39m(tokens)\n\u001b[32m     11\u001b[39m num_t_units = count_t_units(sentence)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnum_words\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_t_units\u001b[49m\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "compute_syntactic_vals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
