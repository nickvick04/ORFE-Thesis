{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "/Users/nickvick/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 356MB/s]                     \n",
      "2026-02-06 15:49:36 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-02-06 15:49:36 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-02-06 15:49:37 INFO: File exists: /Users/nickvick/stanza_resources/en/default.zip\n",
      "2026-02-06 15:49:39 INFO: Finished downloading models and saved to /Users/nickvick/stanza_resources\n",
      "2026-02-06 15:49:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 43.9MB/s]                    \n",
      "2026-02-06 15:49:39 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-02-06 15:49:39 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-02-06 15:49:39 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2026-02-06 15:49:39 INFO: Using device: cpu\n",
      "2026-02-06 15:49:39 INFO: Loading: tokenize\n",
      "2026-02-06 15:49:40 INFO: Loading: mwt\n",
      "2026-02-06 15:49:40 INFO: Loading: pos\n",
      "2026-02-06 15:49:40 INFO: Loading: constituency\n",
      "2026-02-06 15:49:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# syntactic specific imports\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tree import ParentedTree\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('treebank')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained English model\n",
    "\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "stanza_parser = stanza.Pipeline(\"en\", processors=\"tokenize,pos,constituency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# set up for src imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# add project root to sys.path (so src/ can be imported)\n",
    "project_root = os.path.abspath(\"..\")  # adjust if notebooks are nested deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# import required functions\n",
    "from src.data_preprocessing import corpus_to_df, syntactic_preprocessing_df, is_complete_sentence, clean_tokens_lexical, clean_tokens_syntactic, split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/nickvick/.convokit/saved-corpora/subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"subreddit-Cornell\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parented_tree(complete_sent):\n",
    "    '''Helper function to create a tree for a valid sentence'''\n",
    "    \n",
    "    doc = stanza_parser(complete_sent)\n",
    "    stanza_tree = doc.sentences[0].constituency\n",
    "    parented_tree = ParentedTree.fromstring(str(stanza_tree))\n",
    "    \n",
    "    return parented_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_t_units(complete_sent):\n",
    "    '''Helper function that returns the number of t-units in a sentence'''\n",
    "\n",
    "    t_unit_count = 0\n",
    "    is_question = False\n",
    "    counted_s_label = False\n",
    "    has_nested_sq_label = False\n",
    "    parent_label = None\n",
    "    to_decremented = False\n",
    "    \n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(complete_sent)\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "\n",
    "        # extract relevant labels\n",
    "        label = subtree.label()\n",
    "        if subtree.parent():\n",
    "            parent_label = subtree.parent().label()\n",
    "\n",
    "        # flag if the sentence is a question and thus has different rules\n",
    "        if label in {\"SQ\", \"SBARQ\"}:\n",
    "             is_question = True\n",
    "             # if we've counted a preceding S label decrement\n",
    "             if counted_s_label:\n",
    "                t_unit_count -= 1\n",
    "                counted_s_label = False\n",
    "\n",
    "        # logic if sentence is a question\n",
    "        if is_question:\n",
    "            if label == \"SQ\":\n",
    "                t_unit_count += 1\n",
    "                # if nested SQ label, flag\n",
    "                if parent_label == \"SQ\":\n",
    "                    has_nested_sq_label = True\n",
    "\n",
    "        # logic when sentence is not a question\n",
    "        else:\n",
    "            # subtract occurences when \"to\" is considered a new subject\n",
    "            if label == \"TO\" and not to_decremented:\n",
    "                t_unit_count -= 1\n",
    "                to_decremented = True\n",
    "                    \n",
    "            # check for subjects in regular sentences\n",
    "            if label == \"S\":\n",
    "                # if subject belongs to subordinate clause, ignore\n",
    "                if parent_label == \"SBAR\":\n",
    "                    continue\n",
    "                # otherwise increment\n",
    "                counted_s_label = True\n",
    "                t_unit_count += 1\n",
    "    \n",
    "    # ignore duplicated subject labels\n",
    "    if t_unit_count > 1 and not is_question:\n",
    "        t_unit_count -= 1\n",
    "    if has_nested_sq_label:\n",
    "        t_unit_count -= 1\n",
    "\n",
    "    # adjust for inappropriate decrements\n",
    "    if t_unit_count == 0:\n",
    "        if to_decremented:\n",
    "            t_unit_count += 1\n",
    "\n",
    "    # heuristic for special constructions\n",
    "    if t_unit_count == 0:\n",
    "        return 1\n",
    "\n",
    "    return t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_t_units(complete_sent):\n",
    "    '''Helper function that returns a list of the t-units in a complete sentence.'''\n",
    "    \n",
    "    t_units = []\n",
    "    is_question = False\n",
    "    \n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(complete_sent)\n",
    "    \n",
    "    # flag if the sentence is a question\n",
    "    for subtree in ptree.subtrees():\n",
    "        if subtree.label() in {\"SQ\", \"SBARQ\"}:\n",
    "            is_question = True\n",
    "            break\n",
    "    \n",
    "    # extract t-units based on sentence type\n",
    "    if is_question:\n",
    "        # for questions, extract SQ constituents\n",
    "        sq_found = False\n",
    "        for subtree in ptree.subtrees():\n",
    "            if subtree.label() == \"SQ\":\n",
    "                parent_label = subtree.parent().label() if subtree.parent() else None\n",
    "                # skip nested SQ labels\n",
    "                if parent_label == \"SQ\":\n",
    "                    continue\n",
    "                # for top-level SQ, check if it has coordinated SQ children\n",
    "                child_sqs = [child for child in subtree if hasattr(child, 'label') and child.label() == \"SQ\"]\n",
    "                if child_sqs:\n",
    "                    # has coordinated SQ children, extract those instead\n",
    "                    for child_sq in child_sqs:\n",
    "                        t_unit_text = \" \".join(child_sq.leaves())\n",
    "                        t_units.append(t_unit_text)\n",
    "                        sq_found = True\n",
    "                else:\n",
    "                    # no coordinated children, extract this SQ\n",
    "                    t_unit_text = \" \".join(subtree.leaves())\n",
    "                    t_units.append(t_unit_text)\n",
    "                    sq_found = True\n",
    "        \n",
    "        # if no SQ found, fall back to extracting the whole question\n",
    "        if not sq_found:\n",
    "            t_units.append(complete_sent)\n",
    "            \n",
    "    else:\n",
    "        # for declarative sentences, extract S constituents that are direct children of root or coordinated\n",
    "        for subtree in ptree.subtrees():\n",
    "            if subtree.label() == \"S\":\n",
    "                parent_label = subtree.parent().label() if subtree.parent() else None\n",
    "                # skip if subject belongs to subordinate clause\n",
    "                if parent_label == \"SBAR\":\n",
    "                    continue\n",
    "                # skip the top-most S that contains everything\n",
    "                if parent_label in {None, \"ROOT\"} and len([s for s in ptree.subtrees() if s.label() == \"S\"]) > 1:\n",
    "                    continue\n",
    "                t_unit_text = \" \".join(subtree.leaves())\n",
    "                t_units.append(t_unit_text)\n",
    "    \n",
    "    # remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_t_units = []\n",
    "    for t_unit in t_units:\n",
    "        if t_unit not in seen:\n",
    "            seen.add(t_unit)\n",
    "            unique_t_units.append(t_unit)\n",
    "    \n",
    "    # filter out T-units that are only infinitive clauses (start with \"to\" and have no subject)\n",
    "    # keep T-units that have a subject before \"to\" (e.g., \"I want to leave\")\n",
    "    filtered_t_units = []\n",
    "    for t_unit in unique_t_units:\n",
    "        words = t_unit.strip().split()\n",
    "        # if it starts with \"to\", likely an infinitive clause fragment - remove it, unless it's the only t-unit\n",
    "        if words and words[0].lower() == \"to\" and len(unique_t_units) > 1:\n",
    "            continue\n",
    "        filtered_t_units.append(t_unit)\n",
    "    \n",
    "    # heuristic: if no t-units found but it's a complete sentence, return the whole sentence\n",
    "    if len(filtered_t_units) == 0:\n",
    "        return [complete_sent]\n",
    "    \n",
    "    return filtered_t_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clauses(complete_sent):\n",
    "    '''Helper function to count the number of clauses in a complete sentence.'''\n",
    "\n",
    "    clause_count = 0\n",
    "\n",
    "    # only consider complete sentences\n",
    "    if not is_complete_sentence(complete_sent):\n",
    "        return 0\n",
    "    \n",
    "    t_unit_count = count_t_units(complete_sent)\n",
    "\n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(complete_sent)\n",
    "    # print(TreePrettyPrinter(ptree))\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "        # if subject belongs to subordinate clause, increment \n",
    "        if subtree.label() == \"SBAR\":\n",
    "            clause_count += 1\n",
    "\n",
    "    return clause_count + t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_unit_length(t_unit):\n",
    "    '''Helper function that determines the number of tokens in a given t-unit, \n",
    "    a.k.a. the t-unit length '''\n",
    "\n",
    "    tokens = clean_tokens_lexical(t_unit)\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mltu(complete_sentences):\n",
    "    '''Computes the Mean Length of a T-Unit (MLTU) in a particular utterance.'''\n",
    "\n",
    "    # extract the t_units\n",
    "    t_units = []\n",
    "    for sent in complete_sentences:\n",
    "        t_units.append(extract_t_units(sent))\n",
    "    # flatten the t_unit list\n",
    "    t_units = [item for sublist in t_units for item in sublist]\n",
    "\n",
    "    lengths = []\n",
    "    # determine the length of each t-unit\n",
    "    for unit in t_units:\n",
    "        lengths.append(t_unit_length(unit))\n",
    "\n",
    "    return np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_ratio(candidate_sentences, complete_sentences):\n",
    "    '''Function to determine the ratio of fragments to lines in a given text'''\n",
    "\n",
    "    # compute the total number of candidates\n",
    "    total = len(candidate_sentences)\n",
    "    if total == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # find the total number of fragments\n",
    "    num_fragments = total - len(complete_sentences)\n",
    "\n",
    "    return num_fragments / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_t_units_per_sentence(complete_sentences):\n",
    "    '''Function that, given an utterance, computes the number of t_units per sentence \n",
    "    and returns the average across all sentences.'''\n",
    "\n",
    "    # find the number of sentences\n",
    "    num_sentences = len(complete_sentences)\n",
    "\n",
    "    # find the number of t_units\n",
    "    t_units_per_sent = [count_t_units(sent) for sent in complete_sentences]\n",
    "    num_t_units = sum(t_units_per_sent)\n",
    "\n",
    "    return num_t_units / num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clause_t_unit_ratio(complete_sentences):\n",
    "\n",
    "    # find the total number of clauses in the utterance\n",
    "    num_clauses_per_sent = [count_clauses(sent) for sent in complete_sentences]\n",
    "    total_clauses = sum(num_clauses_per_sent)\n",
    "\n",
    "    # find the total number of t_units in the utterance\n",
    "    num_t_units_per_sent = [count_t_units(sent) for sent in complete_sentences]\n",
    "    total_t_units = sum(num_t_units_per_sent)\n",
    "\n",
    "    return total_clauses / total_t_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corpus_to_df(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_syntactic_vals(df):\n",
    "    '''Function to compute the syntactic metrics for each utterance in a dataframe.'''\n",
    "\n",
    "    # convert the text into a list of candidate sentences\n",
    "    df = syntactic_preprocessing_df(df)\n",
    "    num_utterances = len(df)\n",
    "\n",
    "    # initialize lists to store results\n",
    "    fragment_ratio_list = []\n",
    "    avg_t_units_list = []\n",
    "    clause_t_unit_ratio_list = []\n",
    "    mltu_list = []\n",
    "\n",
    "    # compute fragment ratio\n",
    "    for candidate, complete in tqdm(zip(df[\"candidate_sentences\"], df[\"complete_sentences\"]), \n",
    "                                    total=num_utterances, desc=\"Computing fragment ratios\"):\n",
    "        if not candidate:\n",
    "            fragment_ratio_list.append(np.nan)\n",
    "        else:\n",
    "            fragment_ratio_list.append(fragment_ratio(candidate, complete))\n",
    "\n",
    "    # compute average t_units per sentence\n",
    "    for sentences in tqdm(df[\"complete_sentences\"], \n",
    "                          total=num_utterances, desc=\"Computing average t-units per sentence\"):\n",
    "        if not sentences:\n",
    "            avg_t_units_list.append(np.nan)\n",
    "        else:\n",
    "            avg_t_units_list.append(avg_t_units_per_sentence(sentences))\n",
    "\n",
    "\n",
    "    # compute clause to t-unit ratio\n",
    "    for sentences in tqdm(df[\"complete_sentences\"], \n",
    "                          total=num_utterances, desc=\"Computing clause to t-unit ratios\"):\n",
    "        if not sentences:\n",
    "            clause_t_unit_ratio_list.append(np.nan)\n",
    "        else:\n",
    "            clause_t_unit_ratio_list.append(clause_t_unit_ratio(sentences))\n",
    "\n",
    "    # compute mltu values\n",
    "    for sentences in tqdm(df[\"complete_sentences\"], \n",
    "                          total=num_utterances, desc=\"Computing MLTU values\"):\n",
    "        if not sentences:\n",
    "            mltu_list.append(np.nan)\n",
    "        else:\n",
    "            mltu_list.append(mltu(sentences))\n",
    "        \n",
    "    # store all values in dataframe\n",
    "    df[\"fragment_ratio\"] = fragment_ratio_list\n",
    "    df[\"avg_t_units\"] = avg_t_units_list\n",
    "    df[\"clause_to_t_unit_ratio\"] = clause_t_unit_ratio_list\n",
    "    df[\"mltu\"] = mltu_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing syntactic preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fragment ratios: 100%|██████████| 65796/65796 [00:00<00:00, 4221249.79it/s]\n",
      "Computing average t-units per sentence:   0%|          | 210/65796 [01:37<8:28:39,  2.15it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcompute_syntactic_vals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mcompute_syntactic_vals\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     26\u001b[39m         avg_t_units_list.append(np.nan)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         avg_t_units_list.append(\u001b[43mavg_t_units_per_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# compute clause to t-unit ratio\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentences \u001b[38;5;129;01min\u001b[39;00m tqdm(df[\u001b[33m\"\u001b[39m\u001b[33mcomplete_sentences\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m     33\u001b[39m                       total=num_utterances, desc=\u001b[33m\"\u001b[39m\u001b[33mComputing clause to t-unit ratios\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mavg_t_units_per_sentence\u001b[39m\u001b[34m(complete_sentences)\u001b[39m\n\u001b[32m      6\u001b[39m num_sentences = \u001b[38;5;28mlen\u001b[39m(complete_sentences)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# find the number of t_units\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m t_units_per_sent = [\u001b[43mcount_t_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m complete_sentences]\n\u001b[32m     10\u001b[39m num_t_units = \u001b[38;5;28msum\u001b[39m(t_units_per_sent)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m num_t_units / num_sentences\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcount_t_units\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# create a dependency tree\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m ptree = \u001b[43mcreate_parented_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# iterated through parented subtrees\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subtree \u001b[38;5;129;01min\u001b[39;00m ptree.subtrees():\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# extract relevant labels\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcreate_parented_tree\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Helper function to create a tree for a valid sentence'''\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''if not is_complete_sentence(sentence):\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    raise ValueError(\"Sentence is not complete\")'''\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m doc = \u001b[43mstanza_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m stanza_tree = doc.sentences[\u001b[32m0\u001b[39m].constituency\n\u001b[32m      9\u001b[39m parented_tree = ParentedTree.fromstring(\u001b[38;5;28mstr\u001b[39m(stanza_tree))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/pipeline/core.py:480\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, doc, processors)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/pipeline/core.py:431\u001b[39m, in \u001b[36mPipeline.process\u001b[39m\u001b[34m(self, doc, processors)\u001b[39m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processors.get(processor_name):\n\u001b[32m    430\u001b[39m         process = \u001b[38;5;28mself\u001b[39m.processors[processor_name].bulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processors[processor_name].process\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m         doc = \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/pipeline/pos_processor.py:85\u001b[39m, in \u001b[36mPOSProcessor.process\u001b[39m\u001b[34m(self, document)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[32m     84\u001b[39m         idx.extend(b[-\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         preds += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m preds = unsort(preds, idx)\n\u001b[32m     88\u001b[39m dataset.doc.set([doc.UPOS, doc.XPOS, doc.FEATS], [y \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m preds \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/models/pos/trainer.py:98\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, batch, unsort)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m     97\u001b[39m batch_size = word.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m _, preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mufeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_orig_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m upos_seqs = [\u001b[38;5;28mself\u001b[39m.vocab[\u001b[33m'\u001b[39m\u001b[33mupos\u001b[39m\u001b[33m'\u001b[39m].unmap(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m preds[\u001b[32m0\u001b[39m].tolist()]\n\u001b[32m    100\u001b[39m xpos_seqs = [\u001b[38;5;28mself\u001b[39m.vocab[\u001b[33m'\u001b[39m\u001b[33mxpos\u001b[39m\u001b[33m'\u001b[39m].unmap(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m preds[\u001b[32m1\u001b[39m].tolist()]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/models/pos/model.py:169\u001b[39m, in \u001b[36mTagger.forward\u001b[39m\u001b[34m(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\u001b[39m\n\u001b[32m    166\u001b[39m     all_forward_chars = [\u001b[38;5;28mself\u001b[39m.charmodel_forward_transform(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_forward_chars]\n\u001b[32m    167\u001b[39m all_forward_chars = pack(pad_sequence(all_forward_chars, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m all_backward_chars = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcharmodel_backward\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_char_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.charmodel_backward_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    171\u001b[39m     all_backward_chars = [\u001b[38;5;28mself\u001b[39m.charmodel_backward_transform(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_backward_chars]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/models/common/char_model.py:222\u001b[39m, in \u001b[36mCharacterLanguageModel.build_char_representation\u001b[39m\u001b[34m(self, sentences)\u001b[39m\n\u001b[32m    219\u001b[39m chars = get_long_tensor(chars, \u001b[38;5;28mlen\u001b[39m(all_data), pad_id=vocab.unit2id(CHARLM_END)).to(device=device)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     output, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     res = [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_offsets)]\n\u001b[32m    224\u001b[39m     res = unsort(res, orig_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/models/common/char_model.py:153\u001b[39m, in \u001b[36mCharacterLanguageModel.forward\u001b[39m\u001b[34m(self, chars, charlens, hidden)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[32m    151\u001b[39m     hidden = (\u001b[38;5;28mself\u001b[39m.charlstm_h_init.expand(\u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_num_layers\u001b[39m\u001b[33m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_hidden_dim\u001b[39m\u001b[33m'\u001b[39m]).contiguous(),\n\u001b[32m    152\u001b[39m               \u001b[38;5;28mself\u001b[39m.charlstm_c_init.expand(\u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_num_layers\u001b[39m\u001b[33m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mchar_hidden_dim\u001b[39m\u001b[33m'\u001b[39m]).contiguous())\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m output = \u001b[38;5;28mself\u001b[39m.dropout(pad_packed_sequence(output, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m])\n\u001b[32m    155\u001b[39m decoded = \u001b[38;5;28mself\u001b[39m.decoder(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/stanza/models/common/packed_lstm.py:22\u001b[39m, in \u001b[36mPackedLSTM.forward\u001b[39m\u001b[34m(self, input, lengths, hx)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28minput\u001b[39m = pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first=\u001b[38;5;28mself\u001b[39m.batch_first)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pad:\n\u001b[32m     24\u001b[39m     res = (pad_packed_sequence(res[\u001b[32m0\u001b[39m], batch_first=\u001b[38;5;28mself\u001b[39m.batch_first)[\u001b[32m0\u001b[39m], res[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/rnn.py:1139\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1127\u001b[39m     result = _VF.lstm(\n\u001b[32m   1128\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1129\u001b[39m         hx,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1136\u001b[39m         \u001b[38;5;28mself\u001b[39m.batch_first,\n\u001b[32m   1137\u001b[39m     )\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m output = result[\u001b[32m0\u001b[39m]\n\u001b[32m   1151\u001b[39m hidden = result[\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "compute_syntactic_vals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
