{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "/Users/nickvick/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 339MB/s]                     \n",
      "2026-02-06 15:14:45 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-02-06 15:14:45 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-02-06 15:14:46 INFO: File exists: /Users/nickvick/stanza_resources/en/default.zip\n",
      "2026-02-06 15:14:48 INFO: Finished downloading models and saved to /Users/nickvick/stanza_resources\n",
      "2026-02-06 15:14:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 41.2MB/s]                    \n",
      "2026-02-06 15:14:48 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-02-06 15:14:48 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-02-06 15:14:48 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2026-02-06 15:14:48 INFO: Using device: cpu\n",
      "2026-02-06 15:14:48 INFO: Loading: tokenize\n",
      "2026-02-06 15:14:49 INFO: Loading: mwt\n",
      "2026-02-06 15:14:49 INFO: Loading: pos\n",
      "2026-02-06 15:14:49 INFO: Loading: constituency\n",
      "2026-02-06 15:14:50 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# syntactic specific imports\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tree import ParentedTree\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('treebank')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained English model\n",
    "\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "stanza_parser = stanza.Pipeline(\"en\", processors=\"tokenize,pos,constituency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# set up for src imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# add project root to sys.path (so src/ can be imported)\n",
    "project_root = os.path.abspath(\"..\")  # adjust if notebooks are nested deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# import required functions\n",
    "from src.data_preprocessing import corpus_to_df, syntactic_preprocessing_df, is_complete_sentence, clean_tokens_lexical, clean_tokens_syntactic, split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/nickvick/.convokit/saved-corpora/subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"subreddit-Cornell\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parented_tree(sentence):\n",
    "    '''Helper function to create a tree for a valid sentence'''\n",
    "\n",
    "    '''if not is_complete_sentence(sentence):\n",
    "        raise ValueError(\"Sentence is not complete\")'''\n",
    "    \n",
    "    doc = stanza_parser(sentence)\n",
    "    stanza_tree = doc.sentences[0].constituency\n",
    "    parented_tree = ParentedTree.fromstring(str(stanza_tree))\n",
    "    \n",
    "    return parented_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_t_units(sentence):\n",
    "\n",
    "    t_unit_count = 0\n",
    "    is_question = False\n",
    "    counted_s_label = False\n",
    "    has_nested_sq_label = False\n",
    "    parent_label = None\n",
    "    to_decremented = False\n",
    "    is_sentence = is_complete_sentence(sentence)\n",
    "\n",
    "    # if a fragment, there are no t-units\n",
    "    if not is_sentence:\n",
    "        return 0\n",
    "    \n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(sentence)\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "\n",
    "        # extract relevant labels\n",
    "        label = subtree.label()\n",
    "        if subtree.parent():\n",
    "            parent_label = subtree.parent().label()\n",
    "\n",
    "        # flag if the sentence is a question and thus has different rules\n",
    "        if label in {\"SQ\", \"SBARQ\"}:\n",
    "             is_question = True\n",
    "             # if we've counted a preceding S label decrement\n",
    "             if counted_s_label:\n",
    "                t_unit_count -= 1\n",
    "                counted_s_label = False\n",
    "\n",
    "        # logic if sentence is a question\n",
    "        if is_question:\n",
    "            if label == \"SQ\":\n",
    "                t_unit_count += 1\n",
    "                # if nested SQ label, flag\n",
    "                if parent_label == \"SQ\":\n",
    "                    has_nested_sq_label = True\n",
    "\n",
    "        # logic when sentence is not a question\n",
    "        else:\n",
    "            # subtract occurences when \"to\" is considered a new subject\n",
    "            if label == \"TO\" and not to_decremented:\n",
    "                t_unit_count -= 1\n",
    "                to_decremented = True\n",
    "                    \n",
    "            # check for subjects in regular sentences\n",
    "            if label == \"S\":\n",
    "                # if subject belongs to subordinate clause, ignore\n",
    "                if parent_label == \"SBAR\":\n",
    "                    continue\n",
    "                # otherwise increment\n",
    "                counted_s_label = True\n",
    "                t_unit_count += 1\n",
    "    \n",
    "    # ignore duplicated subject labels\n",
    "    if t_unit_count > 1 and not is_question:\n",
    "        t_unit_count -= 1\n",
    "    if has_nested_sq_label:\n",
    "        t_unit_count -= 1\n",
    "\n",
    "    # adjust for inappropriate decrements\n",
    "    if t_unit_count == 0:\n",
    "        if to_decremented:\n",
    "            t_unit_count += 1\n",
    "\n",
    "    # heuristic for special constructions\n",
    "    if t_unit_count == 0 and is_sentence:\n",
    "        return 1\n",
    "\n",
    "\n",
    "    return t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_t_units(sentence):\n",
    "    \n",
    "    t_units = []\n",
    "    is_question = False\n",
    "    is_sentence = is_complete_sentence(sentence)\n",
    "    \n",
    "    # if a fragment, there are no t-units\n",
    "    if not is_sentence:\n",
    "        return []\n",
    "    \n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(sentence)\n",
    "    \n",
    "    # flag if the sentence is a question\n",
    "    for subtree in ptree.subtrees():\n",
    "        if subtree.label() in {\"SQ\", \"SBARQ\"}:\n",
    "            is_question = True\n",
    "            break\n",
    "    \n",
    "    # extract t-units based on sentence type\n",
    "    if is_question:\n",
    "        # for questions, extract SQ constituents\n",
    "        sq_found = False\n",
    "        for subtree in ptree.subtrees():\n",
    "            if subtree.label() == \"SQ\":\n",
    "                parent_label = subtree.parent().label() if subtree.parent() else None\n",
    "                # skip nested SQ labels\n",
    "                if parent_label == \"SQ\":\n",
    "                    continue\n",
    "                # for top-level SQ, check if it has coordinated SQ children\n",
    "                child_sqs = [child for child in subtree if hasattr(child, 'label') and child.label() == \"SQ\"]\n",
    "                if child_sqs:\n",
    "                    # has coordinated SQ children, extract those instead\n",
    "                    for child_sq in child_sqs:\n",
    "                        t_unit_text = \" \".join(child_sq.leaves())\n",
    "                        t_units.append(t_unit_text)\n",
    "                        sq_found = True\n",
    "                else:\n",
    "                    # no coordinated children, extract this SQ\n",
    "                    t_unit_text = \" \".join(subtree.leaves())\n",
    "                    t_units.append(t_unit_text)\n",
    "                    sq_found = True\n",
    "        \n",
    "        # if no SQ found, fall back to extracting the whole question\n",
    "        if not sq_found:\n",
    "            t_units.append(sentence)\n",
    "            \n",
    "    else:\n",
    "        # for declarative sentences, extract S constituents that are direct children of root or coordinated\n",
    "        for subtree in ptree.subtrees():\n",
    "            if subtree.label() == \"S\":\n",
    "                parent_label = subtree.parent().label() if subtree.parent() else None\n",
    "                # skip if subject belongs to subordinate clause\n",
    "                if parent_label == \"SBAR\":\n",
    "                    continue\n",
    "                # skip the top-most S that contains everything\n",
    "                if parent_label in {None, \"ROOT\"} and len([s for s in ptree.subtrees() if s.label() == \"S\"]) > 1:\n",
    "                    continue\n",
    "                t_unit_text = \" \".join(subtree.leaves())\n",
    "                t_units.append(t_unit_text)\n",
    "    \n",
    "    # remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_t_units = []\n",
    "    for t_unit in t_units:\n",
    "        if t_unit not in seen:\n",
    "            seen.add(t_unit)\n",
    "            unique_t_units.append(t_unit)\n",
    "    \n",
    "    # filter out T-units that are only infinitive clauses (start with \"to\" and have no subject)\n",
    "    # keep T-units that have a subject before \"to\" (e.g., \"I want to leave\")\n",
    "    filtered_t_units = []\n",
    "    for t_unit in unique_t_units:\n",
    "        words = t_unit.strip().split()\n",
    "        # if it starts with \"to\", likely an infinitive clause fragment - remove it, unless it's the only t-unit\n",
    "        if words and words[0].lower() == \"to\" and len(unique_t_units) > 1:\n",
    "            continue\n",
    "        filtered_t_units.append(t_unit)\n",
    "    \n",
    "    # heuristic: if no t-units found but it's a complete sentence, return the whole sentence\n",
    "    if len(filtered_t_units) == 0 and is_sentence:\n",
    "        return [sentence]\n",
    "    \n",
    "    return filtered_t_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clauses(sentence):\n",
    "\n",
    "    clause_count = 0\n",
    "\n",
    "    # only consider complete sentences\n",
    "    if not is_complete_sentence(sentence):\n",
    "        return 0\n",
    "    \n",
    "    t_unit_count = count_t_units(sentence)\n",
    "\n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(sentence)\n",
    "    # print(TreePrettyPrinter(ptree))\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "        # if subject belongs to subordinate clause, increment \n",
    "        if subtree.label() == \"SBAR\":\n",
    "            clause_count += 1\n",
    "\n",
    "    return clause_count + t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_unit_length(t_unit):\n",
    "    '''Helper function that determines the number of tokens in a given t-unit, \n",
    "    a.k.a. the t-unit length '''\n",
    "\n",
    "    tokens = clean_tokens_lexical(t_unit)\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mltu(complete_sentences):\n",
    "    '''Computes the Mean Length of a T-Unit (MLTU) in a particular utterance.'''\n",
    "\n",
    "    # extract the t_units\n",
    "    t_units = []\n",
    "    for sent in complete_sentences:\n",
    "        t_units.append(extract_t_units(sent))\n",
    "    # flatten the t_unit list\n",
    "    t_units = [item for sublist in t_units for item in sublist]\n",
    "\n",
    "\n",
    "    lengths = []\n",
    "    # determine the length of each t-unit\n",
    "    for unit in t_units:\n",
    "        lengths.append(t_unit_length(unit))\n",
    "\n",
    "    return np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_ratio(candidate_sentences, complete_sentences):\n",
    "    '''Function to determine the ratio of fragments to lines in a given text'''\n",
    "\n",
    "    # compute the total number of candidates\n",
    "    total = len(candidate_sentences)\n",
    "    if total == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # find the total number of fragments\n",
    "    num_fragments = total - len(complete_sentences)\n",
    "\n",
    "    return num_fragments / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_t_units_per_sentence(complete_sentences):\n",
    "    '''Function that, given an utterance, computes the number of t_units per sentence \n",
    "    and returns the average across all sentences.'''\n",
    "\n",
    "    # find the number of sentences\n",
    "    num_sentences = len(complete_sentences)\n",
    "\n",
    "    # find the number of t_units\n",
    "    t_units_per_sent = [count_t_units(sent) for sent in complete_sentences]\n",
    "    num_t_units = sum(t_units_per_sent)\n",
    "\n",
    "    return num_t_units / num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clause_t_unit_ratio(complete_sentences):\n",
    "\n",
    "    # find the total number of clauses in the utterance\n",
    "    num_clauses_per_sent = [count_clauses(sent) for sent in complete_sentences]\n",
    "    total_clauses = sum(num_clauses_per_sent)\n",
    "\n",
    "    # find the total number of t_units in the utterance\n",
    "    num_t_units_per_sent = [count_t_units(sent) for sent in complete_sentences]\n",
    "    total_t_units = sum(num_t_units_per_sent)\n",
    "\n",
    "    return total_clauses / total_t_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corpus_to_df(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_syntactic_vals(df):\n",
    "    '''Function to compute the syntactic metrics for each utterance in a dataframe.'''\n",
    "\n",
    "    # convert the text into a list of candidate sentences\n",
    "    df = syntactic_preprocessing_df(df)\n",
    "    num_utterances = len(df)\n",
    "\n",
    "    # initialize lists to store results\n",
    "    fragment_ratio_list = []\n",
    "    avg_t_units_list = []\n",
    "    clause_t_unit_ratio_list = []\n",
    "    mltu_list = []\n",
    "\n",
    "    # compute fragment ratio\n",
    "    for candidate, complete in tqdm(zip(df[\"candidate_sentences\"], df[\"complete_sentences\"]), total=num_utterances):\n",
    "        if not candidate:\n",
    "            fragment_ratio_list.append(np.nan)\n",
    "        else:\n",
    "            fragment_ratio_list.append(fragment_ratio(candidate, complete))\n",
    "\n",
    "    # compute remaining metrics\n",
    "    for sentences in tqdm(df[\"complete_sentences\"], total=num_utterances):\n",
    "        if not sentences:\n",
    "            avg_t_units_list.append(np.nan)\n",
    "            clause_t_unit_ratio_list.append(np.nan)\n",
    "            mltu_list.append(np.nan)\n",
    "        else:\n",
    "            avg_t_units_list.append(avg_t_units_per_sentence(sentences))\n",
    "            clause_t_unit_ratio_list.append(clause_t_unit_ratio(sentences))\n",
    "            mltu_list.append(mltu(sentences))\n",
    "        \n",
    "    # store all values in dataframe\n",
    "    df[\"fragment_ratio\"] = fragment_ratio_list\n",
    "    df[\"avg_t_units\"] = avg_t_units_list\n",
    "    df[\"clause_to_t_unit_ratio\"] = clause_t_unit_ratio_list\n",
    "    df[\"mltu\"] = mltu_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickvick/Library/CloudStorage/OneDrive-PrincetonUniversity/ORFE/Thesis/ORFE-Thesis/src/data_preprocessing.py:341: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[\"raw_text\"].str.contains(BOT_TEXT_RE, regex=True)]\n"
     ]
    }
   ],
   "source": [
    "compute_syntactic_vals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
