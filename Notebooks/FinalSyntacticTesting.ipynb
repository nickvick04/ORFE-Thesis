{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 42.4MB/s]                    \n",
      "2026-01-29 14:45:37 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-01-29 14:45:37 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-01-29 14:45:38 INFO: File exists: /Users/nickvick/stanza_resources/en/default.zip\n",
      "2026-01-29 14:45:39 INFO: Finished downloading models and saved to /Users/nickvick/stanza_resources\n",
      "2026-01-29 14:45:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 38.6MB/s]                    \n",
      "2026-01-29 14:45:39 INFO: Downloaded file to /Users/nickvick/stanza_resources/resources.json\n",
      "2026-01-29 14:45:39 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-01-29 14:45:40 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2026-01-29 14:45:40 INFO: Using device: cpu\n",
      "2026-01-29 14:45:40 INFO: Loading: tokenize\n",
      "2026-01-29 14:45:40 INFO: Loading: mwt\n",
      "2026-01-29 14:45:40 INFO: Loading: pos\n",
      "2026-01-29 14:45:41 INFO: Loading: constituency\n",
      "2026-01-29 14:45:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# syntactic specific imports\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tree import ParentedTree\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('treebank')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # pre-trained English model\n",
    "\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "stanza_parser = stanza.Pipeline(\"en\", processors=\"tokenize,pos,constituency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for src imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# add project root to sys.path (so src/ can be imported)\n",
    "project_root = os.path.abspath(\"..\")  # adjust if notebooks are nested deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# import required functions\n",
    "from src.data_preprocessing import corpus_to_df, syntactic_preprocessing_df, is_complete_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parented_tree(sentence):\n",
    "    '''Helper function to create a tree for a valid sentence'''\n",
    "\n",
    "    '''if not is_complete_sentence(sentence):\n",
    "        raise ValueError(\"Sentence is not complete\")'''\n",
    "    \n",
    "    doc = stanza_parser(sentence)\n",
    "    stanza_tree = doc.sentences[0].constituency\n",
    "    parented_tree = ParentedTree.fromstring(str(stanza_tree))\n",
    "    \n",
    "    return parented_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_t_units(sentence):\n",
    "\n",
    "    t_unit_count = 0\n",
    "\n",
    "    # if a fragment, there are no t-units\n",
    "    if not is_complete_sentence(sentence):\n",
    "        return 0\n",
    "    \n",
    "    # create a dependency tree\n",
    "    ptree = create_parented_tree(sentence)\n",
    "\n",
    "    # iterated through parented subtrees\n",
    "    for subtree in ptree.subtrees():\n",
    "\n",
    "        # subtract occurences when \"to\" is considered a new subject\n",
    "        if subtree.label() == \"TO\":\n",
    "                t_unit_count -= 1\n",
    "                \n",
    "        # check for subjects\n",
    "        if subtree.label() == \"S\":\n",
    "            # if subject belongs to subordinate clause, ignore\n",
    "            if subtree.parent().label() == \"SBAR\":\n",
    "                continue\n",
    "            # otherwise increment\n",
    "            t_unit_count += 1\n",
    "    \n",
    "    # if more than one t-unit, ignore duplicated subject below root\n",
    "    if t_unit_count > 1:\n",
    "        t_unit_count -= 1\n",
    "\n",
    "    return t_unit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3022068186.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef count_clauses(sentence):\u001b[39m\n                                ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def count_clauses(sentence):\n",
    "\n",
    "    \n",
    "\n",
    "    clause_count = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_unit_length(sentence):\n",
    "    '''Computes the t-unit length (i.e., the number of words divided by number of t-units)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_ratio(text):\n",
    "    '''Function to determine the ratio of fragments to lines in a given text'''\n",
    "\n",
    "    sentences = split_sentences(text)\n",
    "    total = len(sentences)\n",
    "    if total == 0:\n",
    "        return None\n",
    "\n",
    "    # add complete sentences to a list\n",
    "    is_complete = []\n",
    "    for sent in sentences:\n",
    "        if is_complete_sentence(sent):\n",
    "            is_complete.append(sent)\n",
    "\n",
    "    num_fragment = total - len(is_complete)\n",
    "\n",
    "    fragment_ratio = num_fragment/total\n",
    "\n",
    "    return fragment_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_syntactic_vals(df):\n",
    "    '''Function to compute the syntactic metrics for each utterance in a dataframe.'''\n",
    "\n",
    "    avg_t_units_list = []\n",
    "    clause_t_unit_ratio_list = []\n",
    "    avg_t_unit_length_list = []\n",
    "\n",
    "    for utterance_sentences in tqdm(df[\"final\"]):\n",
    "        # list of values for each sentence\n",
    "        t_units_per_sent = [count_t_units(s) for s in utterance_sentences]\n",
    "        clauses_per_sent = [count_clauses(s) for s in utterance_sentences]\n",
    "        t_unit_lengths_per_sent = [t_unit_length(s) for s in utterance_sentences]\n",
    "\n",
    "        # average per utterance\n",
    "        avg_t_units = sum(t_units_per_sent) / len(t_units_per_sent)\n",
    "        avg_clause_t_unit_ratio = False\n",
    "        avg_t_unit_len = sum(t_unit_lengths_per_sent) / len(t_unit_lengths_per_sent)\n",
    "\n",
    "        # store values for the current utterance\n",
    "        avg_t_units_list.append(avg_t_units)\n",
    "        clause_t_unit_ratio_list.append(avg_clause_t_unit_ratio)\n",
    "        avg_t_unit_length_list.append(avg_t_unit_len)\n",
    "\n",
    "    # store all values in dataframe\n",
    "    df[\"avg_t_units\"] = avg_t_units_list\n",
    "    df[\"clause_to_t_unit_ratio\"] = clause_t_unit_ratio_list\n",
    "    df[\"avg_t_unit_length\"] = avg_t_unit_length_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
