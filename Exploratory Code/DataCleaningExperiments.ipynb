{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# set up nltk tokenizers\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# set up nltk lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# imports specific to lexical measures\n",
    "import re\n",
    "from wordfreq import zipf_frequency\n",
    "from lexical_diversity import lex_div as ld\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/nickvick/.convokit/saved-corpora/subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"subreddit-Cornell\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_PLACEHOLDERS = {\"[deleted]\", \"[removed]\", \"deleted\", \"removed\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''Helper function to tokenize social media text. Note that the TweetTokenizer \n",
    "    preserves mentions, contractions, and other social media-specific structures'''\n",
    "\n",
    "    # handle empty text input\n",
    "    if text is None:\n",
    "        return []\n",
    "    \n",
    "    # return empty list if text is a deleted or removed post\n",
    "    if text in REDDIT_PLACEHOLDERS:\n",
    "        return []\n",
    "\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    '''Helper function to lemmatize tokens.'''\n",
    "\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens_lexical(text):\n",
    "    '''Helper function that tokenizes text, cleans tokens by removing punctuation, numbers, and emojis\n",
    "    for purely lexical analysis, and returns the cleaned, lemmatized tokens.'''\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    # clean tokens\n",
    "    cleaned = []\n",
    "    for tok in tokens:\n",
    "        # skip over punctuation\n",
    "        if re.match(r'^\\W+$', tok):\n",
    "            continue\n",
    "        # skip over emojis\n",
    "\n",
    "        # only keep alphabetic tokens\n",
    "        if tok.isalpha():\n",
    "            cleaned.append(tok.lower())\n",
    "\n",
    "    # lemmatize clean tokens\n",
    "    lemmatized = lemmatize(cleaned)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = list(corpus.iter_utterances())\n",
    "sample_utts = random.sample(utterance, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Called earlier, they just told me to post the ...</td>\n",
       "      <td>[Called, earlier, ,, they, just, told, me, to,...</td>\n",
       "      <td>[Called, earlier, ,, they, just, told, me, to,...</td>\n",
       "      <td>[called, earlier, they, just, told, me, to, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's if you don't pass (in your case you passe...</td>\n",
       "      <td>[It's, if, you, don't, pass, (, in, your, case...</td>\n",
       "      <td>[It's, if, you, don't, pas, (, in, your, case,...</td>\n",
       "      <td>[if, you, pas, in, your, case, you, passed, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Another difficulty is adjusting to the fact th...</td>\n",
       "      <td>[Another, difficulty, is, adjusting, to, the, ...</td>\n",
       "      <td>[Another, difficulty, is, adjusting, to, the, ...</td>\n",
       "      <td>[another, difficulty, is, adjusting, to, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I assume this is in reference to racist commen...</td>\n",
       "      <td>[I, assume, this, is, in, reference, to, racis...</td>\n",
       "      <td>[I, assume, this, is, in, reference, to, racis...</td>\n",
       "      <td>[i, assume, this, is, in, reference, to, racis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeah, unfortunately i'm not sure how to check ...</td>\n",
       "      <td>[yeah, ,, unfortunately, i'm, not, sure, how, ...</td>\n",
       "      <td>[yeah, ,, unfortunately, i'm, not, sure, how, ...</td>\n",
       "      <td>[yeah, unfortunately, not, sure, how, to, chec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lol yes many engineers, including myself, had ...</td>\n",
       "      <td>[Lol, yes, many, engineers, ,, including, myse...</td>\n",
       "      <td>[Lol, yes, many, engineer, ,, including, mysel...</td>\n",
       "      <td>[lol, yes, many, engineer, including, myself, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\+1</td>\n",
       "      <td>[\\, +, 1]</td>\n",
       "      <td>[\\, +, 1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Valid point, but I have read that to be even c...</td>\n",
       "      <td>[Valid, point, ,, but, I, have, read, that, to...</td>\n",
       "      <td>[Valid, point, ,, but, I, have, read, that, to...</td>\n",
       "      <td>[valid, point, but, i, have, read, that, to, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I liked Gia's Elite at Triphammer Mall (via TC...</td>\n",
       "      <td>[I, liked, Gia's, Elite, at, Triphammer, Mall,...</td>\n",
       "      <td>[I, liked, Gia's, Elite, at, Triphammer, Mall,...</td>\n",
       "      <td>[i, liked, elite, at, triphammer, mall, via, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This was me freshman year as well. \\n\\nHonestl...</td>\n",
       "      <td>[This, was, me, freshman, year, as, well, ., H...</td>\n",
       "      <td>[This, wa, me, freshman, year, a, well, ., Hon...</td>\n",
       "      <td>[this, wa, me, freshman, year, a, well, honest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&amp;gt;I've been looking at all the different Int...</td>\n",
       "      <td>[&gt;, I've, been, looking, at, all, the, differe...</td>\n",
       "      <td>[&gt;, I've, been, looking, at, all, the, differe...</td>\n",
       "      <td>[been, looking, at, all, the, different, inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I don't particularly think that taking four co...</td>\n",
       "      <td>[I, don't, particularly, think, that, taking, ...</td>\n",
       "      <td>[I, don't, particularly, think, that, taking, ...</td>\n",
       "      <td>[i, particularly, think, that, taking, four, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CS here is probably easier. Berk probably is s...</td>\n",
       "      <td>[CS, here, is, probably, easier, ., Berk, prob...</td>\n",
       "      <td>[CS, here, is, probably, easier, ., Berk, prob...</td>\n",
       "      <td>[c, here, is, probably, easier, berk, probably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How is the culture at Cornell? Do people usual...</td>\n",
       "      <td>[How, is, the, culture, at, Cornell, ?, Do, pe...</td>\n",
       "      <td>[How, is, the, culture, at, Cornell, ?, Do, pe...</td>\n",
       "      <td>[how, is, the, culture, at, cornell, do, peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>It happens all the fucking time here. How godd...</td>\n",
       "      <td>[It, happens, all, the, fucking, time, here, ....</td>\n",
       "      <td>[It, happens, all, the, fucking, time, here, ....</td>\n",
       "      <td>[it, happens, all, the, fucking, time, here, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I just received an email about \"Becker residen...</td>\n",
       "      <td>[I, just, received, an, email, about, \", Becke...</td>\n",
       "      <td>[I, just, received, an, email, about, \", Becke...</td>\n",
       "      <td>[i, just, received, an, email, about, becker, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Yes, there is this Saturday from 1-4pm.  I'm n...</td>\n",
       "      <td>[Yes, ,, there, is, this, Saturday, from, 1-4,...</td>\n",
       "      <td>[Yes, ,, there, is, this, Saturday, from, 1-4,...</td>\n",
       "      <td>[yes, there, is, this, saturday, from, pm, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The hotel school one is very helpful and usefu...</td>\n",
       "      <td>[The, hotel, school, one, is, very, helpful, a...</td>\n",
       "      <td>[The, hotel, school, one, is, very, helpful, a...</td>\n",
       "      <td>[the, hotel, school, one, is, very, helpful, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_text  \\\n",
       "0   Called earlier, they just told me to post the ...   \n",
       "1   It's if you don't pass (in your case you passe...   \n",
       "2   Another difficulty is adjusting to the fact th...   \n",
       "3   I assume this is in reference to racist commen...   \n",
       "4   yeah, unfortunately i'm not sure how to check ...   \n",
       "5   Lol yes many engineers, including myself, had ...   \n",
       "6                                                 \\+1   \n",
       "7   Valid point, but I have read that to be even c...   \n",
       "8   I liked Gia's Elite at Triphammer Mall (via TC...   \n",
       "9                                           [deleted]   \n",
       "10  This was me freshman year as well. \\n\\nHonestl...   \n",
       "11  &gt;I've been looking at all the different Int...   \n",
       "12  I don't particularly think that taking four co...   \n",
       "13  CS here is probably easier. Berk probably is s...   \n",
       "14                                          [removed]   \n",
       "15  How is the culture at Cornell? Do people usual...   \n",
       "16  It happens all the fucking time here. How godd...   \n",
       "17  I just received an email about \"Becker residen...   \n",
       "18  Yes, there is this Saturday from 1-4pm.  I'm n...   \n",
       "19  The hotel school one is very helpful and usefu...   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [Called, earlier, ,, they, just, told, me, to,...   \n",
       "1   [It's, if, you, don't, pass, (, in, your, case...   \n",
       "2   [Another, difficulty, is, adjusting, to, the, ...   \n",
       "3   [I, assume, this, is, in, reference, to, racis...   \n",
       "4   [yeah, ,, unfortunately, i'm, not, sure, how, ...   \n",
       "5   [Lol, yes, many, engineers, ,, including, myse...   \n",
       "6                                           [\\, +, 1]   \n",
       "7   [Valid, point, ,, but, I, have, read, that, to...   \n",
       "8   [I, liked, Gia's, Elite, at, Triphammer, Mall,...   \n",
       "9                                                  []   \n",
       "10  [This, was, me, freshman, year, as, well, ., H...   \n",
       "11  [>, I've, been, looking, at, all, the, differe...   \n",
       "12  [I, don't, particularly, think, that, taking, ...   \n",
       "13  [CS, here, is, probably, easier, ., Berk, prob...   \n",
       "14                                                 []   \n",
       "15  [How, is, the, culture, at, Cornell, ?, Do, pe...   \n",
       "16  [It, happens, all, the, fucking, time, here, ....   \n",
       "17  [I, just, received, an, email, about, \", Becke...   \n",
       "18  [Yes, ,, there, is, this, Saturday, from, 1-4,...   \n",
       "19  [The, hotel, school, one, is, very, helpful, a...   \n",
       "\n",
       "                                           lemmatized  \\\n",
       "0   [Called, earlier, ,, they, just, told, me, to,...   \n",
       "1   [It's, if, you, don't, pas, (, in, your, case,...   \n",
       "2   [Another, difficulty, is, adjusting, to, the, ...   \n",
       "3   [I, assume, this, is, in, reference, to, racis...   \n",
       "4   [yeah, ,, unfortunately, i'm, not, sure, how, ...   \n",
       "5   [Lol, yes, many, engineer, ,, including, mysel...   \n",
       "6                                           [\\, +, 1]   \n",
       "7   [Valid, point, ,, but, I, have, read, that, to...   \n",
       "8   [I, liked, Gia's, Elite, at, Triphammer, Mall,...   \n",
       "9                                                  []   \n",
       "10  [This, wa, me, freshman, year, a, well, ., Hon...   \n",
       "11  [>, I've, been, looking, at, all, the, differe...   \n",
       "12  [I, don't, particularly, think, that, taking, ...   \n",
       "13  [CS, here, is, probably, easier, ., Berk, prob...   \n",
       "14                                                 []   \n",
       "15  [How, is, the, culture, at, Cornell, ?, Do, pe...   \n",
       "16  [It, happens, all, the, fucking, time, here, ....   \n",
       "17  [I, just, received, an, email, about, \", Becke...   \n",
       "18  [Yes, ,, there, is, this, Saturday, from, 1-4,...   \n",
       "19  [The, hotel, school, one, is, very, helpful, a...   \n",
       "\n",
       "                                              cleaned  \n",
       "0   [called, earlier, they, just, told, me, to, po...  \n",
       "1   [if, you, pas, in, your, case, you, passed, th...  \n",
       "2   [another, difficulty, is, adjusting, to, the, ...  \n",
       "3   [i, assume, this, is, in, reference, to, racis...  \n",
       "4   [yeah, unfortunately, not, sure, how, to, chec...  \n",
       "5   [lol, yes, many, engineer, including, myself, ...  \n",
       "6                                                  []  \n",
       "7   [valid, point, but, i, have, read, that, to, b...  \n",
       "8   [i, liked, elite, at, triphammer, mall, via, t...  \n",
       "9                                                  []  \n",
       "10  [this, wa, me, freshman, year, a, well, honest...  \n",
       "11  [been, looking, at, all, the, different, inter...  \n",
       "12  [i, particularly, think, that, taking, four, c...  \n",
       "13  [c, here, is, probably, easier, berk, probably...  \n",
       "14                                                 []  \n",
       "15  [how, is, the, culture, at, cornell, do, peopl...  \n",
       "16  [it, happens, all, the, fucking, time, here, h...  \n",
       "17  [i, just, received, an, email, about, becker, ...  \n",
       "18  [yes, there, is, this, saturday, from, pm, not...  \n",
       "19  [the, hotel, school, one, is, very, helpful, a...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for utt in sample_utts:\n",
    "    text = utt.text\n",
    "    rows.append({\n",
    "        \"raw_text\": text,\n",
    "        \"tokens\": tokenize(text),\n",
    "        \"lemmatized\": lemmatize(tokenize(text)),\n",
    "        \"cleaned\": clean_tokens_lexical(text)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
