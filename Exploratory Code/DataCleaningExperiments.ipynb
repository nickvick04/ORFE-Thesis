{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nickvick/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# set up nltk tokenizers\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# set up nltk lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# imports specific to lexical measures\n",
    "import re\n",
    "from wordfreq import zipf_frequency\n",
    "from lexical_diversity import lex_div as ld\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/nickvick/.convokit/saved-corpora/subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"subreddit-Cornell\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_df(corpus):\n",
    "\n",
    "    data = []\n",
    "    for utt in corpus.iter_utterances():\n",
    "        # only consider utterances with timestamps and text\n",
    "        if hasattr(utt, \"timestamp\") and utt.text:\n",
    "            # convert timestamp from seconds since 1/1/1970 to datetime\n",
    "            t = datetime.fromtimestamp(int(utt.timestamp))\n",
    "\n",
    "            data.append({\n",
    "                \"utterance_id\": utt.id,\n",
    "                \"speaker_id\": utt.speaker.id,\n",
    "                \"text\": utt.text,\n",
    "                \"timestamp\": t\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Level Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_TEXT_PATTERNS = [\n",
    "    r\"\\bi am a bot\\b\",\n",
    "    r\"\\bthis (comment|post) was (posted|left by) a bot\",\n",
    "    r\"\\bthis reply was generated automatically\",\n",
    "    r\"[\\^*]*beep(?:\\s+beep)?[\\^*]*\\s+[\\^*]*boop(?:\\s+boop)?[\\^*]*\"\n",
    "]\n",
    "\n",
    "BOT_TEXT_RE = re.compile(\"|\".join(BOT_TEXT_PATTERNS), flags=re.IGNORECASE)\n",
    "URL_RE = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "HAS_LETTER_RE = re.compile(r\"[A-Za-z]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_lexical(text):\n",
    "\n",
    "    # remove emojis\n",
    "\n",
    "    # remove urls\n",
    "    text = URL_RE.sub(\"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''Helper function to tokenize social media text. Note that the TweetTokenizer \n",
    "    preserves mentions, contractions, and other social media-specific structures'''\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    '''Helper function to lemmatize tokens.'''\n",
    "\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens_lexical(text):\n",
    "    '''Helper function that tokenizes text, cleans tokens by removing punctuation, numbers, and emojis\n",
    "    for purely lexical analysis, and returns the cleaned, lemmatized tokens.'''\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = tokenize(text)\n",
    "\n",
    "    # clean tokens\n",
    "    cleaned = []\n",
    "    for tok in tokens:\n",
    "        # skip over punctuation\n",
    "        if re.match(r'^\\W+$', tok):\n",
    "            continue\n",
    "        # skip over emojis\n",
    "\n",
    "        # only keep alphabetic tokens\n",
    "        if tok.isalpha():\n",
    "            cleaned.append(tok.lower())\n",
    "\n",
    "    # lemmatize clean tokens\n",
    "    lemmatized = lemmatize(cleaned)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # remove deleted/removed utterances\n",
    "    df = df[~df[\"text\"].str.lower().isin({\"[deleted]\", \"[removed]\"})]\n",
    "\n",
    "    # remove bot authored utterances\n",
    "    df = df[~df[\"text\"].str.contains(BOT_TEXT_RE)]\n",
    "\n",
    "    # remove utterances without a letter\n",
    "    df = df[df[\"text\"].str.contains(HAS_LETTER_RE)]\n",
    "\n",
    "    # clean text\n",
    "    df[\"clean_text\"] = df[\"text\"].apply(clean_text_lexical)\n",
    "\n",
    "    # tokenize\n",
    "    df[\"tokens\"] = df[\"clean_text\"].apply(tokenize)\n",
    "\n",
    "    # lemmatize\n",
    "    df[\"lemmas\"] = df[\"tokens\"].apply(lemmatize)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance = list(corpus.iter_utterances())\n",
    "sample_utts = random.sample(utterance, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dj/6y_wgvgs5m59w08whyk9sjp80000gn/T/ipykernel_71839/1308788558.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[~df[\"text\"].str.contains(BOT_TEXT_RE)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nyx4d</td>\n",
       "      <td>reddmau5</td>\n",
       "      <td>I was just reading about the Princeton Mic-Che...</td>\n",
       "      <td>2012-01-01 16:18:18</td>\n",
       "      <td>I was just reading about the Princeton Mic-Che...</td>\n",
       "      <td>[I, was, just, reading, about, the, Princeton,...</td>\n",
       "      <td>[I, wa, just, reading, about, the, Princeton, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o0145</td>\n",
       "      <td>shtylman</td>\n",
       "      <td>I have added support for Cornell to courseoff....</td>\n",
       "      <td>2012-01-02 13:57:15</td>\n",
       "      <td>I have added support for Cornell to courseoff....</td>\n",
       "      <td>[I, have, added, support, for, Cornell, to, co...</td>\n",
       "      <td>[I, have, added, support, for, Cornell, to, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o1gca</td>\n",
       "      <td>moon_river</td>\n",
       "      <td>i don't have a facebook, so we'd need a volunt...</td>\n",
       "      <td>2012-01-03 14:55:06</td>\n",
       "      <td>i don't have a facebook, so we'd need a volunt...</td>\n",
       "      <td>[i, don't, have, a, facebook, ,, so, we'd, nee...</td>\n",
       "      <td>[i, don't, have, a, facebook, ,, so, we'd, nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o0ss4</td>\n",
       "      <td>moon_river</td>\n",
       "      <td>so, i'm starting to mess with some of the css ...</td>\n",
       "      <td>2012-01-03 01:16:17</td>\n",
       "      <td>so, i'm starting to mess with some of the css ...</td>\n",
       "      <td>[so, ,, i'm, starting, to, mess, with, some, o...</td>\n",
       "      <td>[so, ,, i'm, starting, to, mess, with, some, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o4ipd</td>\n",
       "      <td>reddmau5</td>\n",
       "      <td>Ever since SOPA put fear into the hearts of ev...</td>\n",
       "      <td>2012-01-05 17:08:06</td>\n",
       "      <td>Ever since SOPA put fear into the hearts of ev...</td>\n",
       "      <td>[Ever, since, SOPA, put, fear, into, the, hear...</td>\n",
       "      <td>[Ever, since, SOPA, put, fear, into, the, hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72981</th>\n",
       "      <td>e8tj9ig</td>\n",
       "      <td>JCsurfing</td>\n",
       "      <td>If your ECs are good, maybe.</td>\n",
       "      <td>2018-10-31 19:39:29</td>\n",
       "      <td>If your ECs are good, maybe.</td>\n",
       "      <td>[If, your, ECs, are, good, ,, maybe, .]</td>\n",
       "      <td>[If, your, ECs, are, good, ,, maybe, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72983</th>\n",
       "      <td>e8tjyg1</td>\n",
       "      <td>ultimatefishlover</td>\n",
       "      <td>harvard is the  **i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞è̶͚͙̳̩...</td>\n",
       "      <td>2018-10-31 19:50:51</td>\n",
       "      <td>harvard is the  **i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞è̶͚͙̳̩...</td>\n",
       "      <td>[harvard, is, the, *, *, i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞...</td>\n",
       "      <td>[harvard, is, the, *, *, i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72984</th>\n",
       "      <td>e8tkb66</td>\n",
       "      <td>KickAssEmployee</td>\n",
       "      <td>Agreed</td>\n",
       "      <td>2018-10-31 19:56:45</td>\n",
       "      <td>Agreed</td>\n",
       "      <td>[Agreed]</td>\n",
       "      <td>[Agreed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72985</th>\n",
       "      <td>e8tkctl</td>\n",
       "      <td>dasfsadf123</td>\n",
       "      <td>Why did this make me laugh so hard ahaahha</td>\n",
       "      <td>2018-10-31 19:57:30</td>\n",
       "      <td>Why did this make me laugh so hard ahaahha</td>\n",
       "      <td>[Why, did, this, make, me, laugh, so, hard, ah...</td>\n",
       "      <td>[Why, did, this, make, me, laugh, so, hard, ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72986</th>\n",
       "      <td>e8tkgl1</td>\n",
       "      <td>dasfsadf123</td>\n",
       "      <td>\"crimson\" just call it dark red you pretentiou...</td>\n",
       "      <td>2018-10-31 19:59:16</td>\n",
       "      <td>\"crimson\" just call it dark red you pretentiou...</td>\n",
       "      <td>[\", crimson, \", just, call, it, dark, red, you...</td>\n",
       "      <td>[\", crimson, \", just, call, it, dark, red, you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65796 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      utterance_id         speaker_id  \\\n",
       "0            nyx4d           reddmau5   \n",
       "1            o0145           shtylman   \n",
       "2            o1gca         moon_river   \n",
       "3            o0ss4         moon_river   \n",
       "4            o4ipd           reddmau5   \n",
       "...            ...                ...   \n",
       "72981      e8tj9ig          JCsurfing   \n",
       "72983      e8tjyg1  ultimatefishlover   \n",
       "72984      e8tkb66    KickAssEmployee   \n",
       "72985      e8tkctl        dasfsadf123   \n",
       "72986      e8tkgl1        dasfsadf123   \n",
       "\n",
       "                                                    text           timestamp  \\\n",
       "0      I was just reading about the Princeton Mic-Che... 2012-01-01 16:18:18   \n",
       "1      I have added support for Cornell to courseoff.... 2012-01-02 13:57:15   \n",
       "2      i don't have a facebook, so we'd need a volunt... 2012-01-03 14:55:06   \n",
       "3      so, i'm starting to mess with some of the css ... 2012-01-03 01:16:17   \n",
       "4      Ever since SOPA put fear into the hearts of ev... 2012-01-05 17:08:06   \n",
       "...                                                  ...                 ...   \n",
       "72981                       If your ECs are good, maybe. 2018-10-31 19:39:29   \n",
       "72983  harvard is the  **i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞è̶͚͙̳̩... 2018-10-31 19:50:51   \n",
       "72984                                             Agreed 2018-10-31 19:56:45   \n",
       "72985         Why did this make me laugh so hard ahaahha 2018-10-31 19:57:30   \n",
       "72986  \"crimson\" just call it dark red you pretentiou... 2018-10-31 19:59:16   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0      I was just reading about the Princeton Mic-Che...   \n",
       "1      I have added support for Cornell to courseoff....   \n",
       "2      i don't have a facebook, so we'd need a volunt...   \n",
       "3      so, i'm starting to mess with some of the css ...   \n",
       "4      Ever since SOPA put fear into the hearts of ev...   \n",
       "...                                                  ...   \n",
       "72981                       If your ECs are good, maybe.   \n",
       "72983  harvard is the  **i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞è̶͚͙̳̩...   \n",
       "72984                                             Agreed   \n",
       "72985         Why did this make me laugh so hard ahaahha   \n",
       "72986  \"crimson\" just call it dark red you pretentiou...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [I, was, just, reading, about, the, Princeton,...   \n",
       "1      [I, have, added, support, for, Cornell, to, co...   \n",
       "2      [i, don't, have, a, facebook, ,, so, we'd, nee...   \n",
       "3      [so, ,, i'm, starting, to, mess, with, some, o...   \n",
       "4      [Ever, since, SOPA, put, fear, into, the, hear...   \n",
       "...                                                  ...   \n",
       "72981            [If, your, ECs, are, good, ,, maybe, .]   \n",
       "72983  [harvard, is, the, *, *, i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞...   \n",
       "72984                                           [Agreed]   \n",
       "72985  [Why, did, this, make, me, laugh, so, hard, ah...   \n",
       "72986  [\", crimson, \", just, call, it, dark, red, you...   \n",
       "\n",
       "                                                  lemmas  \n",
       "0      [I, wa, just, reading, about, the, Princeton, ...  \n",
       "1      [I, have, added, support, for, Cornell, to, co...  \n",
       "2      [i, don't, have, a, facebook, ,, so, we'd, nee...  \n",
       "3      [so, ,, i'm, starting, to, mess, with, some, o...  \n",
       "4      [Ever, since, SOPA, put, fear, into, the, hear...  \n",
       "...                                                  ...  \n",
       "72981            [If, your, ECs, are, good, ,, maybe, .]  \n",
       "72983  [harvard, is, the, *, *, i̼͕̻͓̩̘͟n͙̞̙f̶̢̙̻̺͍̟͞...  \n",
       "72984                                           [Agreed]  \n",
       "72985  [Why, did, this, make, me, laugh, so, hard, ah...  \n",
       "72986  [\", crimson, \", just, call, it, dark, red, you...  \n",
       "\n",
       "[65796 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = corpus_to_df(corpus)\n",
    "df = preprocess_df(df)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
